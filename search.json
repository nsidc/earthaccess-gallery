[
  {
    "objectID": "notebooks/getting-started.html#overview",
    "href": "notebooks/getting-started.html#overview",
    "title": "Getting Started with earthaccess",
    "section": "Overview",
    "text": "Overview\nThis tutorial analyzes global sea level rise from satellite altimetry data and compares it to a historic record. We will be reproducing the plot from this infographic: NASA-led Study Reveals the Causes of Sea Level Rise Since 1900."
  },
  {
    "objectID": "notebooks/getting-started.html#learning-objectives",
    "href": "notebooks/getting-started.html#learning-objectives",
    "title": "Getting Started with earthaccess",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSearch for data programmatically using keywords or datasets’ concept_id\nAccess data using the earthaccess python library\nVisualize sea level rise trends from altimetry datasets and compare against historic records"
  },
  {
    "objectID": "notebooks/getting-started.html#requirements",
    "href": "notebooks/getting-started.html#requirements",
    "title": "Getting Started with earthaccess",
    "section": "Requirements",
    "text": "Requirements\n\n1. Compute environment\n\nThis notebook can run anywhere thanks to earthaccess!\n\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nImport (or Install) Packages\n\nimport logging\nlogging.basicConfig(level=logging.INFO,\n                    force = True)\n\ntry:\n    import earthaccess\n    import xarray as xr\n    from pyproj import Geod\n    import numpy as np\n    import hvplot.xarray\n    from matplotlib import pyplot as plt\n    from pprint import pprint\n    import panel as pn\n    import panel.widgets as pnw\nexcept ImportError as e:\n    logging.warning(\"installing missing dependencies... \")\n    %pip install -q earthaccess matplotlib hvplot pyproj xarray numpy h5netcdf panel\nfinally:\n    import earthaccess\n    import xarray as xr\n    from pyproj import Geod\n    import numpy as np\n    import hvplot.xarray\n    from matplotlib import pyplot as plt\n    from pprint import pprint\n    import panel.widgets as pnw\n    logging.info(\"Dependencies imported\")"
  },
  {
    "objectID": "notebooks/getting-started.html#earthaccess-and-nasas-edl",
    "href": "notebooks/getting-started.html#earthaccess-and-nasas-edl",
    "title": "Getting Started with earthaccess",
    "section": "earthaccess and NASA’s EDL",
    "text": "earthaccess and NASA’s EDL\nWe recommend authenticating your Earthdata Login (EDL) information using the earthaccess python package as follows:\n\nauth = earthaccess.login()"
  },
  {
    "objectID": "notebooks/getting-started.html#search-for-sea-surface-height-data",
    "href": "notebooks/getting-started.html#search-for-sea-surface-height-data",
    "title": "Getting Started with earthaccess",
    "section": "Search for Sea Surface Height Data",
    "text": "Search for Sea Surface Height Data\nLet’s find the first four collections that match a keyword search for Sea Surface Height and print out the first two.\n\nmax_results = 10\n\ncollections = earthaccess.search_datasets(\n    keyword = \"SEA SURFACE HEIGHT\",\n    cloud_hosted = True,\n    count = max_results\n)\n\n# Let's print 2 collections\nfor collection in collections[0:2]:\n    # pprint(collection.summary())\n    print(pprint(collection.summary()), collection.abstract(), \"\\n\", collection[\"umm\"][\"DOI\"], \"\\n\\n\")"
  },
  {
    "objectID": "notebooks/getting-started.html#access-data",
    "href": "notebooks/getting-started.html#access-data",
    "title": "Getting Started with earthaccess",
    "section": "Access Data",
    "text": "Access Data\nThe first dataset looks like it contains data from many altimetry missions, let’s explore a bit more! We will access the first granule of the SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205 collection in the month of May for every year that data is available and open the granules via xarray.\n\ngranules = []\n\nfor year in range(1990, 2020):\n    print(f\"Searching for data in {year}\")\n    granule = earthaccess.search_data(doi=\"10.5067/SLREF-CDRV3\",\n                                      temporal= (f\"{year}-05\", f\"{year}-06\"),\n                                      count = 1)\n    if len(granule)&gt;0:\n        granules.append(granule[0])\nprint(f\"Total granules: {len(granules)}\")     \n\n\n%%time\n\nds_SSH = xr.open_mfdataset(earthaccess.open(granules), chunks={})\nds_SSH"
  },
  {
    "objectID": "notebooks/getting-started.html#plot-sea-level-anomalies",
    "href": "notebooks/getting-started.html#plot-sea-level-anomalies",
    "title": "Getting Started with earthaccess",
    "section": "Plot Sea Level Anomalies",
    "text": "Plot Sea Level Anomalies\n\n\ntime = pnw.Player(name='Time', start=0, end=len(ds_SSH.Time)-1, loop_policy='loop', interval=1000)\n\nds_SSH.SLA.interactive(loc='bottom', aspect=\"equal\").isel(Time=time).hvplot(cmap=\"inferno\", data_aspect=1)"
  },
  {
    "objectID": "notebooks/getting-started.html#recreate-the-sea-level-rise-infographic",
    "href": "notebooks/getting-started.html#recreate-the-sea-level-rise-infographic",
    "title": "Getting Started with earthaccess",
    "section": "Recreate the Sea Level Rise Infographic",
    "text": "Recreate the Sea Level Rise Infographic\nFirst, we define a function that will calculate the area in 1/6 by 1/6 degree boxes in order to calculate the global mean of the SSH later.\n\ndef ssl_area(lats):\n    \"\"\"\n    Calculate the area associated with a 1/6 by 1/6 degree box at latitude specified in 'lats'.\n    \n    Parameter\n    ==========\n    lats: a list or numpy array of size N the latitudes of interest. \n    \n    Return\n    =======\n    out: Array (N) area values (unit: m^2)\n    \"\"\"\n    # Define WGS84 as CRS:\n    geod = Geod(ellps='WGS84')\n    dx=1/12.0\n    # create a lambda function for calculating the perimeters of the boxes\n    c_area=lambda lat: geod.polygon_area_perimeter(np.r_[-dx,dx,dx,-dx], lat+np.r_[-dx,-dx,dx,dx])[0]\n    out=[]\n    for lat in lats:\n        out.append(c_area(lat))\n    return np.array(out)\n\nLet’s use the function on our Sea Surface Height dataset.\n\n# note: they rotated the data in the last release, this operation used to be (1,-1)\nssh_area = ssl_area(ds_SSH.Latitude.data).reshape(-1,1)\nprint(ssh_area.shape)\n\nNext, we find and open the historic record dataset also using earthaccess and xarray.\n\nhistoric_ts_results = earthaccess.search_data(short_name='JPL_RECON_GMSL')\nhistoric_ts=xr.open_mfdataset(earthaccess.open([historic_ts_results[0]]), engine='h5netcdf')\nhistoric_ts"
  },
  {
    "objectID": "notebooks/getting-started.html#lets-plot",
    "href": "notebooks/getting-started.html#lets-plot",
    "title": "Getting Started with earthaccess",
    "section": "Let’s Plot!",
    "text": "Let’s Plot!\n\n%%time\n\nplt.rcParams[\"figure.figsize\"] = (16,4)\n\nfig, axs = plt.subplots()\nplt.grid(True)\n\n#function to get the global mean for plotting\ndef global_mean(SLA, **kwargs):\n    dout=((SLA*ssh_area).sum()/(SLA/SLA*ssh_area).sum())*1000\n    return dout\n\nresult = ds_SSH.SLA.groupby('Time').apply(global_mean)\n\nplt.xlabel('Time (year)',fontsize=16)\nplt.ylabel('Global Mean SLA (meter)',fontsize=12)\nplt.grid(True)\n\nresult.plot(ax=axs, color=\"orange\", marker=\"o\", label='Satellite Record')\n\nhistoric_ts['global_average_sea_level_change'].plot(ax=axs, label='Historical in-situ record', color=\"lightblue\")\n\nplt.legend()\nplt.show()\n\n\nThis Data Story is based on Jinbo Wang’s Earthdata Webinar tutorial."
  },
  {
    "objectID": "notebooks/earthaccess-kerchunk.html",
    "href": "notebooks/earthaccess-kerchunk.html",
    "title": "On-demand Zarr Stores for NASA datasets with earthaccess and Kerchunk",
    "section": "",
    "text": "The idea behind this PR from James Borbeau on earthaccess is that we can combine earthaccess, the power of Dask and kerchunk to create consolidated refenrece files (zarr compatible) from NASA datasets. This method works best with gridded data as they can be combined by time using the same grid.\nNotes: * Looks like the resulting consolidated store has coordinate encoding issues for some datasets, as this study form the HDF Group notes, Kerchunk is still on an early phase and doesn’t support all the features of HDF5. * Lucas Sterzinger notes that further optimizations are possible for big datasets. * Having a distributed cluster means that we could scale trhis approach and create on-demand Zarr views of NASA data. A more detailed description of what Kerchunk buys us can be found on this notebook from Lucas.\n\n%%capture\n!pip uninstall -y earthaccess\n!pip install git+https://github.com/jrbourbeau/earthaccess.git@kerchunk\n\n\nExample with SSTS, gridded global NetCDF\n\nimport earthaccess\nimport xarray as xr\nfrom dask.distributed import LocalCluster\n\nif __name__ == \"__main__\":\n\n    # Authenticate my machine with `earthaccess`\n    earthaccess.login()\n\n    # Retrieve data files for the dataset I'm interested in\n    short_name = \"SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205\"\n    granuales = earthaccess.search_data(\n        short_name=short_name,\n        cloud_hosted=True,\n        temporal=(\"1990\", \"2019\"),\n        count=10,  # For demo purposes\n    )\n\n    # Create a local Dask cluster for parallel metadata consolidation\n    # (but works with any Dask cluster)\n    cluster = LocalCluster()\n    client = cluster.get_client()\n\n    # Save consolidated metdata file\n    outfile = earthaccess.consolidate_metadata(\n        granuales,\n        outfile=f\"./{short_name}-metadata.json\",    # Writing to a local file for demo purposes\n        # outfile=f\"s3://my-bucket/{short_name}-metadata.json\",   # We could also write to a remote file\n        access=\"indirect\",\n        kerchunk_options={\"concat_dims\": \"Time\"}\n    )\n    print(f\"Consolidated metadata written to {outfile}\")\n\n    # Load the dataset using the consolidated metadata file\n    fs = earthaccess.get_fsspec_https_session()\n    ds = xr.open_dataset(\n        \"reference://\",\n        engine=\"zarr\",\n        chunks={},\n        backend_kwargs={\n            \"consolidated\": False,\n            \"storage_options\": {\n                \"fo\": outfile,\n                \"remote_protocol\": \"https\",\n                \"remote_options\": fs.storage_options,\n            }\n        },\n    )\n\n    result = ds.SLA.mean({\"Latitude\", \"Longitude\"}).compute()\n    print(f\"{result = }\")\n\n\n\nUsing Chelle’s Tutorial for MUR SST on AWS as reference to build a Zarr store from 10 years of monthly data from MUR.\n\nif __name__ == \"__main__\":\n\n    # Authenticate my machine with `earthaccess`\n    earthaccess.login()\n \n    doi = \"10.5067/GHGMR-4FJ04\"\n    short_name = \"MUR\"\n    month = 7\n    \n    results = []\n    \n    for year in range(2012,2022):\n    \n        params = {\n            \"doi\": doi,\n            \"cloud_hosted\": True,\n            \"temporal\": (f\"{str(year)}-{str(month)}-01\", f\"{str(year)}-{str(month)}-31\"),\n            \"count\": 31\n        }\n\n        # Retrieve data files for the dataset I'm interested in\n        print(f\"Searching for granules on {year}\")\n        granules = earthaccess.search_data(**params)\n        results.extend(granules)\n    print(f\"Total granules to process: {len(results)}\")\n\n    # Create a local Dask cluster for parallel metadata consolidation\n    # (but works with any Dask cluster)\n    cluster = LocalCluster()\n    client = cluster.get_client()\n\n    # Save consolidated metdata file\n    outfile = earthaccess.consolidate_metadata(\n        results,\n        outfile=f\"./direct-{short_name}-metadata.json\",    # Writing to a local file for demo purposes\n        # outfile=f\"s3://my-bucket/{short_name}-metadata.json\",   # We could also write to a remote file\n        access=\"direct\",\n        # kerchunk_options={\"coo_map\": []}\n        kerchunk_options={\"concat_dims\": \"time\"}\n    )\n    print(f\"Consolidated metadata written to {outfile}\")\n\nEARTHDATA_USERNAME and EARTHDATA_PASSWORD are not set in the current environment, try setting them or use a different strategy (netrc, interactive)\nYou're now authenticated with NASA Earthdata Login\nUsing token with expiration date: 10/23/2023\nUsing .netrc file for EDL\nSearching for granules on 2012\nGranules found: 31\nSearching for granules on 2013\nGranules found: 31\nSearching for granules on 2014\nGranules found: 31\nSearching for granules on 2015\nGranules found: 31\nSearching for granules on 2016\nGranules found: 31\nSearching for granules on 2017\nGranules found: 31\nSearching for granules on 2018\nGranules found: 31\nSearching for granules on 2019\nGranules found: 31\nSearching for granules on 2020\nGranules found: 31\nSearching for granules on 2021\nGranules found: 31\nTotal granules to process: 310\nConsolidated metadata written to ./direct-MUR-metadata.json\n\n\n\nearthaccess.login()\n\nfs = earthaccess.get_s3fs_session(\"GES_DISC\")\n\nds = xr.open_dataset(\n    \"reference://\",\n    engine=\"zarr\",\n    chunks={},\n    decode_coords=False, # tricky, the coords are there but encoded in a way xarray can't decode for some reason. Similar to https://github.com/fsspec/kerchunk/issues/177\n    backend_kwargs={\n        \"consolidated\": False,\n        \"storage_options\": {\n            \"fo\": \"direct-MUR-metadata.json\",\n            \"remote_protocol\": \"s3\",\n            \"remote_options\": fs.storage_options,\n        }\n    },\n)\nds"
  },
  {
    "objectID": "notebooks/custom-maps.html",
    "href": "notebooks/custom-maps.html",
    "title": "earthaccess gallery",
    "section": "",
    "text": "import earthaccess\nimport ipyleaflet\nfrom ipywidgets import Dropdown\nfrom shapely.geometry.polygon import orient\nfrom shapely.geometry import Polygon\n\nauth = earthaccess.login()\n\n\n# ArcticDEM\n# note that we need to use the same projection for the image service layer and the map.\nm1 = ipyleaflet.Map(\n    center=(90, 0),\n    zoom=4,\n    basemap=ipyleaflet.basemaps.Esri.ArcticOceanBase,\n    crs=ipyleaflet.projections.EPSG5936.ESRIBasemap,\n)\n# add arctic ocean reference basemap\ntl1 = ipyleaflet.basemap_to_tiles(ipyleaflet.basemaps.Esri.ArcticOceanReference)\nm1.add(tl1)\n\n# create a widget control for the raster function\nraster_functions = [\n    \"Aspect Map\",\n    \"Contour 25\",\n    \"Hillshade Elevation Tinted\",\n    \"Hillshade Gray\",\n    \"Height Ellipsoidal\",\n    \"Height Orthometric\",\n    \"Slope Map\"]\nraster_dropdown1 = Dropdown(\n    value=raster_functions[3],\n    options=raster_functions,\n    description=\"Raster:\",\n)\n\n# add image service layer with ArcticDEM\nurl = 'https://elevation2.arcgis.com/arcgis/rest/services/Polar/ArcticDEM/ImageServer'\nrendering_rule = {\"rasterFunction\": raster_dropdown1.value}\nimage_layer = ipyleaflet.ImageService(\n    name=\"CustomBaseLayer\",\n    url=url,\n    format='jpgpng', rendering_rule=rendering_rule,\n    attribution='Esri, PGC, UMN, NSF, NGA, DigitalGlobe',\n    crs=ipyleaflet.projections.EPSG5936.ESRIBasemap)\n\nm1.add(image_layer) \n\n# add control for raster function\ndropdown_control = ipyleaflet.WidgetControl(widget=raster_dropdown1, position=\"topright\")\nm1.add(dropdown_control)\n\n# set the rendering rule\ndef set_raster_function(sender):\n    image_layer.rendering_rule = {\"rasterFunction\": raster_dropdown1.value}\n    # force redrawing of map by removing and adding layer\n    m1.remove(image_layer)\n    m1.add(image_layer)\n\n\n# watch raster function widget for changes\nraster_dropdown1.observe(set_raster_function)\n\n\n# polygon = [(-83.837578, 45.82529), (-80.167794, 45.82529), (-80.167794, 46.021921), (-83.837578, 46.021921), (-83.837578, 45.82529)]\ncoords = [(-139.59, 59.53), (-138.80, 59.90), (-137.15, 59.45),(-137.30, 58.78),(-138.64, 59.18),(-139.59, 59.53)]\npolygon = list(orient(Polygon(coords)).exterior.coords)\n\n\nif 'sw' in vars():\n    if len(sw.roi) &gt; 0:\n        polygon = sw.roi\nelse:\n    sw = earthaccess.SearchWidget(map=m1)\n\n\nparams = {\n    \"concept_id\" : [\"C2596864127-NSIDC_CPRD\", \"C2076090826-LPCLOUD\", \"C2237824918-ORNL_CLOUD\"],\n    \"temporal\": (\"2020-01\", \"2023-09\"),\n    # \"cloud_cover\": (0, 20),\n    \"polygon\": polygon\n}\nresults = earthaccess.search_data(**params)\nm = sw.explore(results, roi={\"polygon\": polygon})\n# explore will inject its own controls so we add the base layer dropdown back\ntry:\n    sw.map.add(dropdown_control)\nexcept Exception:\n    pass\nm\n\nGranules found: 226"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n\n\nExamples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at nsidc@nsidc.org. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\nFor answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-responsibilities",
    "href": "CODE_OF_CONDUCT.html#our-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at nsidc@nsidc.org. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\nFor answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq"
  },
  {
    "objectID": "notebooks/explore.html",
    "href": "notebooks/explore.html",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "",
    "text": "TL;DR: earthaccess is uses NASA APIs to search, preview and access NASA datasets on-prem and in the cloud with 4 lines of Python.\nThere are many ways to access NASA datasets, we can use the NASA’s Earthdata search portal. We can use DAAC specific websites or tools. We could even use data.gov! These web portals are great but… they are not designed for programmatic access and reproducible workflows. This is extremely important in the age of the cloud and reproducible open science. In this context, earthaccess aims to be a simple library that can deal with the important parts of the metadata so we can access or download data without having to worry if a given dataset is on-prem or in the cloud.\nThe core function of auth is to deal with cloud credentials and remote file sessions (fsspec or requests). essentially, anything that requires you to log in to Earthdata. Most of this will happen behind-the-scenes for you once you have been authenticated.\n\n%pip uninstall -y earthaccess\n%pip install git+https://github.com/nsidc/earthaccess.git@explore\n\n\n\n\nStep 1. We need to open an account with NASA Eardtada, this credentials will allow us to access NASA datasets.\n\nOnce we have our account we can use it with earthaccess\n\nimport earthaccess\n\nauth = earthaccess.login()"
  },
  {
    "objectID": "notebooks/explore.html#overview",
    "href": "notebooks/explore.html#overview",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "",
    "text": "TL;DR: earthaccess is uses NASA APIs to search, preview and access NASA datasets on-prem and in the cloud with 4 lines of Python.\nThere are many ways to access NASA datasets, we can use the NASA’s Earthdata search portal. We can use DAAC specific websites or tools. We could even use data.gov! These web portals are great but… they are not designed for programmatic access and reproducible workflows. This is extremely important in the age of the cloud and reproducible open science. In this context, earthaccess aims to be a simple library that can deal with the important parts of the metadata so we can access or download data without having to worry if a given dataset is on-prem or in the cloud.\nThe core function of auth is to deal with cloud credentials and remote file sessions (fsspec or requests). essentially, anything that requires you to log in to Earthdata. Most of this will happen behind-the-scenes for you once you have been authenticated.\n\n%pip uninstall -y earthaccess\n%pip install git+https://github.com/nsidc/earthaccess.git@explore\n\n\n\n\nStep 1. We need to open an account with NASA Eardtada, this credentials will allow us to access NASA datasets.\n\nOnce we have our account we can use it with earthaccess\n\nimport earthaccess\n\nauth = earthaccess.login()"
  },
  {
    "objectID": "notebooks/explore.html#searching-for-data-using-a-region-of-interest",
    "href": "notebooks/explore.html#searching-for-data-using-a-region-of-interest",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "Searching for data using a region of interest",
    "text": "Searching for data using a region of interest\n\npath = \"bosque_primavera.json\"\n# path = \"bosque_primavera.kml\" \n# path = \"bosque_primavera.shp\"\ngeom = earthaccess.load_geometry(path)"
  },
  {
    "objectID": "notebooks/explore.html#search-and-access-with-earthaccess",
    "href": "notebooks/explore.html#search-and-access-with-earthaccess",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "Search and Access with earthaccess",
    "text": "Search and Access with earthaccess\nearthaccess uses NASA’s search API to search for data from the different Distributed Archive Centers, the data can be hosted by the DAACs or in AWS, with earthaccess we don’t need to think about this because it will handle the authentication for us. For reproducible workflows we just need to use the dataset (or collection as NASA calls them) concept_id.\nThe concept_id of a collection can be found with earthaccess or using NASA Earthdata search portal.\n\nresults = earthaccess.search_data(\n    concept_id = [\"C2613553260-NSIDC_CPRD\", \"C2237824918-ORNL_CLOUD\", \"C1908348134-LPDAAC_ECS\", \"C2021957657-LPCLOUD\", \"C2631841556-LPCLOUD\"],\n    temporal = (\"2013\", \"2023\"),\n    # unpacking the dict\n    **geom\n)\n\nGranules found: 1905\n\n\n\nm = earthaccess.explore(results, roi=geom)\nm"
  },
  {
    "objectID": "notebooks/explore.html#accessing-the-data-with-.download-and-.open",
    "href": "notebooks/explore.html#accessing-the-data-with-.download-and-.open",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "Accessing the data with .download() and .open()",
    "text": "Accessing the data with .download() and .open()\n\nOption 1. I’m not in AWS\n\n\nOption 2. I’m in AWS us-west-2 ☁️\n\nAnalysis in place with S3 direct access\nSame API, just a different origin\n\n%%time\n\nresults = earthaccess.search_data(\n    concept_id = [\"C2021957657-LPCLOUD\"],\n    temporal = (\"2013\", \"2023\"),\n    # unpacking the dict\n    **geom\n)\n\n# files = earthaccess.open()\n\nGranules found: 472\nCPU times: user 212 ms, sys: 19.1 ms, total: 231 ms\nWall time: 3.62 s\n\n\n\n%%time\nfiles = earthaccess.open(results[0:4])\n\nOpening 4 granules, approx size: 0.75 GB\nCPU times: user 591 ms, sys: 63.5 ms, total: 654 ms\nWall time: 7.91 s\n\n\n\n\n\n\n\n\n\n\n\n\nimport rioxarray\n\nds = rioxarray.open_rasterio(files[0])\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 3660, x: 3660)&gt;\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6e+05 6.001e+05 ... 7.098e+05 7.098e+05\n  * y            (y) float64 2.4e+06 2.4e+06 2.4e+06 ... 2.29e+06 2.29e+06\n    spatial_ref  int64 0\nAttributes: (12/36)\n    ACCODE:                    Lasrc; Lasrc\n    add_offset:                0.0\n    AREA_OR_POINT:             Area\n    arop_ave_xshift(meters):   0, 0\n    arop_ave_yshift(meters):   0, 0\n    arop_ncp:                  0, 0\n    ...                        ...\n    TIRS_SSM_MODEL:            ACTUAL; ACTUAL\n    TIRS_SSM_POSITION_STATUS:  NOMINAL; NOMINAL\n    ULX:                       600000\n    ULY:                       2400000\n    USGS_SOFTWARE:             LPGS_15.3.1c\n    _FillValue:                -9999xarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float646e+05 6e+05 ... 7.098e+05 7.098e+05array([600015., 600045., 600075., ..., 709725., 709755., 709785.])y(y)float642.4e+06 2.4e+06 ... 2.29e+06array([2399985., 2399955., 2399925., ..., 2290275., 2290245., 2290215.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not specified (based on WGS 84 spheroid)projected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :600000.0 30.0 0.0 2400000.0 0.0 -30.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([600015.0, 600045.0, 600075.0, 600105.0, 600135.0, 600165.0, 600195.0,\n       600225.0, 600255.0, 600285.0,\n       ...\n       709515.0, 709545.0, 709575.0, 709605.0, 709635.0, 709665.0, 709695.0,\n       709725.0, 709755.0, 709785.0],\n      dtype='float64', name='x', length=3660))yPandasIndexPandasIndex(Index([2399985.0, 2399955.0, 2399925.0, 2399895.0, 2399865.0, 2399835.0,\n       2399805.0, 2399775.0, 2399745.0, 2399715.0,\n       ...\n       2290485.0, 2290455.0, 2290425.0, 2290395.0, 2290365.0, 2290335.0,\n       2290305.0, 2290275.0, 2290245.0, 2290215.0],\n      dtype='float64', name='y', length=3660))Attributes: (36)ACCODE :Lasrc; Lasrcadd_offset :0.0AREA_OR_POINT :Areaarop_ave_xshift(meters) :0, 0arop_ave_yshift(meters) :0, 0arop_ncp :0, 0arop_rmse(meters) :0, 0arop_s2_refimg :NONEcloud_coverage :14HLS_PROCESSING_TIME :2022-03-28T18:44:47ZHORIZONTAL_CS_NAME :UTM, WGS84, UTM ZONE 13; UTM, WGS84, UTM ZONE 13L1_PROCESSING_TIME :2020-09-12T20:22:38Z; 2020-09-12T20:38:53ZLANDSAT_PRODUCT_ID :LC08_L1TP_029045_20130610_20200912_02_T1; LC08_L1TP_029046_20130610_20200912_02_T1LANDSAT_SCENE_ID :LC80290452013161LGN02; LC80290462013161LGN02long_name :RedMEAN_SUN_AZIMUTH_ANGLE :80.7093371815872MEAN_SUN_ZENITH_ANGLE :21.7385427039501MEAN_VIEW_AZIMUTH_ANGLE :135.04059864629MEAN_VIEW_ZENITH_ANGLE :3.32796752185848NBAR_SOLAR_ZENITH :19.9568287674254NCOLS :3660NROWS :3660OVR_RESAMPLING_ALG :NEARESTPROCESSING_LEVEL :L1TP; L1TPscale_factor :0.0001SENSING_TIME :2013-06-10T17:19:45.9657940Z; 2013-06-10T17:20:09.8610670ZSENSOR :OLI_TIRS; OLI_TIRSSENTINEL2_TILEID :13QFDspatial_coverage :100SPATIAL_RESOLUTION :30TIRS_SSM_MODEL :ACTUAL; ACTUALTIRS_SSM_POSITION_STATUS :NOMINAL; NOMINALULX :600000ULY :2400000USGS_SOFTWARE :LPGS_15.3.1c_FillValue :-9999\n\n\n\nds.clip()\n\n&lt;matplotlib.collections.QuadMesh at 0x7f53d7a7a5e0&gt;\n\n\n\n\n\n\ngeometries = [\n    {\n        'type': 'Polygon',\n        'coordinates':[geom[\"polygon\"]]\n    }\n]\nclipped = ds.rio.clip(geometries, drop=True, crs=4326)\nclipped\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 71, x: 570)&gt;\narray([[[-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        [-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        [-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        ...,\n        [-9999,   796,   796, ...,  1016,   792, -9999],\n        [  719,   794,   815, ...,   861,   760,   942],\n        [  617,   710,   785, ...,   798,   842,  1165]]], dtype=int16)\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6.404e+05 6.405e+05 ... 6.575e+05 6.575e+05\n  * y            (y) float64 2.292e+06 2.292e+06 2.292e+06 ... 2.29e+06 2.29e+06\n    spatial_ref  int64 0\nAttributes: (12/36)\n    ACCODE:                    Lasrc; Lasrc\n    add_offset:                0.0\n    AREA_OR_POINT:             Area\n    arop_ave_xshift(meters):   0, 0\n    arop_ave_yshift(meters):   0, 0\n    arop_ncp:                  0, 0\n    ...                        ...\n    TIRS_SSM_MODEL:            ACTUAL; ACTUAL\n    TIRS_SSM_POSITION_STATUS:  NOMINAL; NOMINAL\n    ULX:                       600000\n    ULY:                       2400000\n    USGS_SOFTWARE:             LPGS_15.3.1c\n    _FillValue:                -9999xarray.DataArrayband: 1y: 71x: 570-9999 -9999 -9999 -9999 -9999 -9999 -9999 ... 846 999 971 798 842 1165array([[[-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        [-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        [-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        ...,\n        [-9999,   796,   796, ...,  1016,   792, -9999],\n        [  719,   794,   815, ...,   861,   760,   942],\n        [  617,   710,   785, ...,   798,   842,  1165]]], dtype=int16)Coordinates: (4)band(band)int641array([1])x(x)float646.404e+05 6.405e+05 ... 6.575e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([640425., 640455., 640485., ..., 657435., 657465., 657495.])y(y)float642.292e+06 2.292e+06 ... 2.29e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([2292315., 2292285., 2292255., 2292225., 2292195., 2292165., 2292135.,\n       2292105., 2292075., 2292045., 2292015., 2291985., 2291955., 2291925.,\n       2291895., 2291865., 2291835., 2291805., 2291775., 2291745., 2291715.,\n       2291685., 2291655., 2291625., 2291595., 2291565., 2291535., 2291505.,\n       2291475., 2291445., 2291415., 2291385., 2291355., 2291325., 2291295.,\n       2291265., 2291235., 2291205., 2291175., 2291145., 2291115., 2291085.,\n       2291055., 2291025., 2290995., 2290965., 2290935., 2290905., 2290875.,\n       2290845., 2290815., 2290785., 2290755., 2290725., 2290695., 2290665.,\n       2290635., 2290605., 2290575., 2290545., 2290515., 2290485., 2290455.,\n       2290425., 2290395., 2290365., 2290335., 2290305., 2290275., 2290245.,\n       2290215.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not specified (based on WGS 84 spheroid)projected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :640410.0 30.0 0.0 2292330.0 0.0 -30.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([640425.0, 640455.0, 640485.0, 640515.0, 640545.0, 640575.0, 640605.0,\n       640635.0, 640665.0, 640695.0,\n       ...\n       657225.0, 657255.0, 657285.0, 657315.0, 657345.0, 657375.0, 657405.0,\n       657435.0, 657465.0, 657495.0],\n      dtype='float64', name='x', length=570))yPandasIndexPandasIndex(Index([2292315.0, 2292285.0, 2292255.0, 2292225.0, 2292195.0, 2292165.0,\n       2292135.0, 2292105.0, 2292075.0, 2292045.0, 2292015.0, 2291985.0,\n       2291955.0, 2291925.0, 2291895.0, 2291865.0, 2291835.0, 2291805.0,\n       2291775.0, 2291745.0, 2291715.0, 2291685.0, 2291655.0, 2291625.0,\n       2291595.0, 2291565.0, 2291535.0, 2291505.0, 2291475.0, 2291445.0,\n       2291415.0, 2291385.0, 2291355.0, 2291325.0, 2291295.0, 2291265.0,\n       2291235.0, 2291205.0, 2291175.0, 2291145.0, 2291115.0, 2291085.0,\n       2291055.0, 2291025.0, 2290995.0, 2290965.0, 2290935.0, 2290905.0,\n       2290875.0, 2290845.0, 2290815.0, 2290785.0, 2290755.0, 2290725.0,\n       2290695.0, 2290665.0, 2290635.0, 2290605.0, 2290575.0, 2290545.0,\n       2290515.0, 2290485.0, 2290455.0, 2290425.0, 2290395.0, 2290365.0,\n       2290335.0, 2290305.0, 2290275.0, 2290245.0, 2290215.0],\n      dtype='float64', name='y'))Attributes: (36)ACCODE :Lasrc; Lasrcadd_offset :0.0AREA_OR_POINT :Areaarop_ave_xshift(meters) :0, 0arop_ave_yshift(meters) :0, 0arop_ncp :0, 0arop_rmse(meters) :0, 0arop_s2_refimg :NONEcloud_coverage :14HLS_PROCESSING_TIME :2022-03-28T18:44:47ZHORIZONTAL_CS_NAME :UTM, WGS84, UTM ZONE 13; UTM, WGS84, UTM ZONE 13L1_PROCESSING_TIME :2020-09-12T20:22:38Z; 2020-09-12T20:38:53ZLANDSAT_PRODUCT_ID :LC08_L1TP_029045_20130610_20200912_02_T1; LC08_L1TP_029046_20130610_20200912_02_T1LANDSAT_SCENE_ID :LC80290452013161LGN02; LC80290462013161LGN02long_name :RedMEAN_SUN_AZIMUTH_ANGLE :80.7093371815872MEAN_SUN_ZENITH_ANGLE :21.7385427039501MEAN_VIEW_AZIMUTH_ANGLE :135.04059864629MEAN_VIEW_ZENITH_ANGLE :3.32796752185848NBAR_SOLAR_ZENITH :19.9568287674254NCOLS :3660NROWS :3660OVR_RESAMPLING_ALG :NEARESTPROCESSING_LEVEL :L1TP; L1TPscale_factor :0.0001SENSING_TIME :2013-06-10T17:19:45.9657940Z; 2013-06-10T17:20:09.8610670ZSENSOR :OLI_TIRS; OLI_TIRSSENTINEL2_TILEID :13QFDspatial_coverage :100SPATIAL_RESOLUTION :30TIRS_SSM_MODEL :ACTUAL; ACTUALTIRS_SSM_POSITION_STATUS :NOMINAL; NOMINALULX :600000ULY :2400000USGS_SOFTWARE :LPGS_15.3.1c_FillValue :-9999\n\n\n\nclipped.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x7f5395c88580&gt;\n\n\n\n\n\n\n\nclipped.hvplot(x=\"x\", y=\"y\", crs=xds.rio.estimate_utm_crs()) * map"
  },
  {
    "objectID": "notebooks/explore.html#next-step-subsetting-in-the-cloud",
    "href": "notebooks/explore.html#next-step-subsetting-in-the-cloud",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "Next Step: Subsetting in the Cloud",
    "text": "Next Step: Subsetting in the Cloud\n\n%%time\n# accessing the data on prem means downloading it if we are in a local environment or \"uploading them\" if we are in the cloud.\norder = earthaccess.subset(results, roi=polygon)\n\n\nRelated links\nCMR API documentation: https://cmr.earthaccess.nasa.gov/search/site/docs/search/api.html\nEDL API documentation: https://urs.earthaccess.nasa.gov/\nNASA OpenScapes: https://nasa-openscapes.github.io/earthaccess-cloud-cookbook/\nNSIDC: https://nsidc.org"
  }
]