[
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n\n\nExamples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n\n\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at nsidc@nsidc.org. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership.\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\nFor answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing respectful of differing viewpoints and experiences\nGracefully accepting constructive criticism\nFocusing on what is best for the community\nShowing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\nThe use of sexualized language or imagery and unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-responsibilities",
    "href": "CODE_OF_CONDUCT.html#our-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at nsidc@nsidc.org. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project’s leadership."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\nFor answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq"
  },
  {
    "objectID": "notebooks/earthaccess-kerchunk.html",
    "href": "notebooks/earthaccess-kerchunk.html",
    "title": "On-demand Zarr Stores for NASA datasets with earthaccess and Kerchunk",
    "section": "",
    "text": "The idea behind this PR from James Borbeau on earthaccess is that we can combine earthaccess, the power of Dask and kerchunk to create consolidated refenrece files (zarr compatible) from NASA datasets. This method works best with gridded data as they can be combined by time using the same grid.\nNotes: * Looks like the resulting consolidated store has coordinate encoding issues for some datasets, as this study form the HDF Group notes, Kerchunk is still on an early phase and doesn’t support all the features of HDF5. * Lucas Sterzinger notes that further optimizations are possible for big datasets. * Having a distributed cluster means that we could scale trhis approach and create on-demand Zarr views of NASA data. A more detailed description of what Kerchunk buys us can be found on this notebook from Lucas.\n\n%%capture\n!pip uninstall -y earthaccess\n!pip install git+https://github.com/jrbourbeau/earthaccess.git@kerchunk\n\n\nExample with SSTS, gridded global NetCDF\n\nimport earthaccess\nimport xarray as xr\nfrom dask.distributed import LocalCluster\n\nif __name__ == \"__main__\":\n\n    # Authenticate my machine with `earthaccess`\n    earthaccess.login()\n\n    # Retrieve data files for the dataset I'm interested in\n    short_name = \"SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205\"\n    granuales = earthaccess.search_data(\n        short_name=short_name,\n        cloud_hosted=True,\n        temporal=(\"1990\", \"2019\"),\n        count=10,  # For demo purposes\n    )\n\n    # Create a local Dask cluster for parallel metadata consolidation\n    # (but works with any Dask cluster)\n    cluster = LocalCluster()\n    client = cluster.get_client()\n\n    # Save consolidated metdata file\n    outfile = earthaccess.consolidate_metadata(\n        granuales,\n        outfile=f\"./{short_name}-metadata.json\",    # Writing to a local file for demo purposes\n        # outfile=f\"s3://my-bucket/{short_name}-metadata.json\",   # We could also write to a remote file\n        access=\"indirect\",\n        kerchunk_options={\"concat_dims\": \"Time\"}\n    )\n    print(f\"Consolidated metadata written to {outfile}\")\n\n    # Load the dataset using the consolidated metadata file\n    fs = earthaccess.get_fsspec_https_session()\n    ds = xr.open_dataset(\n        \"reference://\",\n        engine=\"zarr\",\n        chunks={},\n        backend_kwargs={\n            \"consolidated\": False,\n            \"storage_options\": {\n                \"fo\": outfile,\n                \"remote_protocol\": \"https\",\n                \"remote_options\": fs.storage_options,\n            }\n        },\n    )\n\n    result = ds.SLA.mean({\"Latitude\", \"Longitude\"}).compute()\n    print(f\"{result = }\")\n\n\n\nUsing Chelle’s Tutorial for MUR SST on AWS as reference to build a Zarr store from 10 years of monthly data from MUR.\n\nif __name__ == \"__main__\":\n\n    # Authenticate my machine with `earthaccess`\n    earthaccess.login()\n \n    doi = \"10.5067/GHGMR-4FJ04\"\n    short_name = \"MUR\"\n    month = 7\n    \n    results = []\n    \n    for year in range(2012,2022):\n    \n        params = {\n            \"doi\": doi,\n            \"cloud_hosted\": True,\n            \"temporal\": (f\"{str(year)}-{str(month)}-01\", f\"{str(year)}-{str(month)}-31\"),\n            \"count\": 31\n        }\n\n        # Retrieve data files for the dataset I'm interested in\n        print(f\"Searching for granules on {year}\")\n        granules = earthaccess.search_data(**params)\n        results.extend(granules)\n    print(f\"Total granules to process: {len(results)}\")\n\n    # Create a local Dask cluster for parallel metadata consolidation\n    # (but works with any Dask cluster)\n    cluster = LocalCluster()\n    client = cluster.get_client()\n\n    # Save consolidated metdata file\n    outfile = earthaccess.consolidate_metadata(\n        results,\n        outfile=f\"./direct-{short_name}-metadata.json\",    # Writing to a local file for demo purposes\n        # outfile=f\"s3://my-bucket/{short_name}-metadata.json\",   # We could also write to a remote file\n        access=\"direct\",\n        # kerchunk_options={\"coo_map\": []}\n        kerchunk_options={\"concat_dims\": \"time\"}\n    )\n    print(f\"Consolidated metadata written to {outfile}\")\n\nEARTHDATA_USERNAME and EARTHDATA_PASSWORD are not set in the current environment, try setting them or use a different strategy (netrc, interactive)\nYou're now authenticated with NASA Earthdata Login\nUsing token with expiration date: 10/23/2023\nUsing .netrc file for EDL\nSearching for granules on 2012\nGranules found: 31\nSearching for granules on 2013\nGranules found: 31\nSearching for granules on 2014\nGranules found: 31\nSearching for granules on 2015\nGranules found: 31\nSearching for granules on 2016\nGranules found: 31\nSearching for granules on 2017\nGranules found: 31\nSearching for granules on 2018\nGranules found: 31\nSearching for granules on 2019\nGranules found: 31\nSearching for granules on 2020\nGranules found: 31\nSearching for granules on 2021\nGranules found: 31\nTotal granules to process: 310\nConsolidated metadata written to ./direct-MUR-metadata.json\n\n\n\nearthaccess.login()\n\nfs = earthaccess.get_s3fs_session(\"GES_DISC\")\n\nds = xr.open_dataset(\n    \"reference://\",\n    engine=\"zarr\",\n    chunks={},\n    decode_coords=False, # tricky, the coords are there but encoded in a way xarray can't decode for some reason. Similar to https://github.com/fsspec/kerchunk/issues/177\n    backend_kwargs={\n        \"consolidated\": False,\n        \"storage_options\": {\n            \"fo\": \"direct-MUR-metadata.json\",\n            \"remote_protocol\": \"s3\",\n            \"remote_options\": fs.storage_options,\n        }\n    },\n)\nds"
  },
  {
    "objectID": "notebooks/explore.html",
    "href": "notebooks/explore.html",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "",
    "text": "TL;DR: earthaccess is uses NASA APIs to search, preview and access NASA datasets on-prem and in the cloud with 4 lines of Python.\nThere are many ways to access NASA datasets, we can use the NASA’s Earthdata search portal. We can use DAAC specific websites or tools. We could even use data.gov! These web portals are great but… they are not designed for programmatic access and reproducible workflows. This is extremely important in the age of the cloud and reproducible open science. In this context, earthaccess aims to be a simple library that can deal with the important parts of the metadata so we can access or download data without having to worry if a given dataset is on-prem or in the cloud.\nThe core function of auth is to deal with cloud credentials and remote file sessions (fsspec or requests). essentially, anything that requires you to log in to Earthdata. Most of this will happen behind-the-scenes for you once you have been authenticated.\n\n%pip uninstall -y earthaccess\n%pip install git+https://github.com/nsidc/earthaccess.git@explore\n\n\n\n\nStep 1. We need to open an account with NASA Eardtada, this credentials will allow us to access NASA datasets.\n\nOnce we have our account we can use it with earthaccess\n\nimport earthaccess\n\nauth = earthaccess.login()"
  },
  {
    "objectID": "notebooks/explore.html#overview",
    "href": "notebooks/explore.html#overview",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "",
    "text": "TL;DR: earthaccess is uses NASA APIs to search, preview and access NASA datasets on-prem and in the cloud with 4 lines of Python.\nThere are many ways to access NASA datasets, we can use the NASA’s Earthdata search portal. We can use DAAC specific websites or tools. We could even use data.gov! These web portals are great but… they are not designed for programmatic access and reproducible workflows. This is extremely important in the age of the cloud and reproducible open science. In this context, earthaccess aims to be a simple library that can deal with the important parts of the metadata so we can access or download data without having to worry if a given dataset is on-prem or in the cloud.\nThe core function of auth is to deal with cloud credentials and remote file sessions (fsspec or requests). essentially, anything that requires you to log in to Earthdata. Most of this will happen behind-the-scenes for you once you have been authenticated.\n\n%pip uninstall -y earthaccess\n%pip install git+https://github.com/nsidc/earthaccess.git@explore\n\n\n\n\nStep 1. We need to open an account with NASA Eardtada, this credentials will allow us to access NASA datasets.\n\nOnce we have our account we can use it with earthaccess\n\nimport earthaccess\n\nauth = earthaccess.login()"
  },
  {
    "objectID": "notebooks/explore.html#searching-for-data-using-a-region-of-interest",
    "href": "notebooks/explore.html#searching-for-data-using-a-region-of-interest",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "Searching for data using a region of interest",
    "text": "Searching for data using a region of interest\n\npath = \"bosque_primavera.json\"\n# path = \"bosque_primavera.kml\" \n# path = \"bosque_primavera.shp\"\ngeom = earthaccess.load_geometry(path)"
  },
  {
    "objectID": "notebooks/explore.html#search-and-access-with-earthaccess",
    "href": "notebooks/explore.html#search-and-access-with-earthaccess",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "Search and Access with earthaccess",
    "text": "Search and Access with earthaccess\nearthaccess uses NASA’s search API to search for data from the different Distributed Archive Centers, the data can be hosted by the DAACs or in AWS, with earthaccess we don’t need to think about this because it will handle the authentication for us. For reproducible workflows we just need to use the dataset (or collection as NASA calls them) concept_id.\nThe concept_id of a collection can be found with earthaccess or using NASA Earthdata search portal.\n\nresults = earthaccess.search_data(\n    concept_id = [\"C2613553260-NSIDC_CPRD\", \"C2237824918-ORNL_CLOUD\", \"C1908348134-LPDAAC_ECS\", \"C2021957657-LPCLOUD\", \"C2631841556-LPCLOUD\"],\n    temporal = (\"2013\", \"2023\"),\n    # unpacking the dict\n    **geom\n)\n\nGranules found: 1905\n\n\n\nm = earthaccess.explore(results, roi=geom)\nm"
  },
  {
    "objectID": "notebooks/explore.html#accessing-the-data-with-.download-and-.open",
    "href": "notebooks/explore.html#accessing-the-data-with-.download-and-.open",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "Accessing the data with .download() and .open()",
    "text": "Accessing the data with .download() and .open()\n\nOption 1. I’m not in AWS\n\n\nOption 2. I’m in AWS us-west-2 ☁️\n\nAnalysis in place with S3 direct access\nSame API, just a different origin\n\n%%time\n\nresults = earthaccess.search_data(\n    concept_id = [\"C2021957657-LPCLOUD\"],\n    temporal = (\"2013\", \"2023\"),\n    # unpacking the dict\n    **geom\n)\n\n# files = earthaccess.open()\n\nGranules found: 472\nCPU times: user 212 ms, sys: 19.1 ms, total: 231 ms\nWall time: 3.62 s\n\n\n\n%%time\nfiles = earthaccess.open(results[0:4])\n\nOpening 4 granules, approx size: 0.75 GB\nCPU times: user 591 ms, sys: 63.5 ms, total: 654 ms\nWall time: 7.91 s\n\n\n\n\n\n\n\n\n\n\n\n\nimport rioxarray\n\nds = rioxarray.open_rasterio(files[0])\nds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 3660, x: 3660)&gt;\n[13395600 values with dtype=int16]\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6e+05 6e+05 6.001e+05 ... 7.098e+05 7.098e+05\n  * y            (y) float64 2.4e+06 2.4e+06 2.4e+06 ... 2.29e+06 2.29e+06\n    spatial_ref  int64 0\nAttributes: (12/36)\n    ACCODE:                    Lasrc; Lasrc\n    add_offset:                0.0\n    AREA_OR_POINT:             Area\n    arop_ave_xshift(meters):   0, 0\n    arop_ave_yshift(meters):   0, 0\n    arop_ncp:                  0, 0\n    ...                        ...\n    TIRS_SSM_MODEL:            ACTUAL; ACTUAL\n    TIRS_SSM_POSITION_STATUS:  NOMINAL; NOMINAL\n    ULX:                       600000\n    ULY:                       2400000\n    USGS_SOFTWARE:             LPGS_15.3.1c\n    _FillValue:                -9999xarray.DataArrayband: 1y: 3660x: 3660...[13395600 values with dtype=int16]Coordinates: (4)band(band)int641array([1])x(x)float646e+05 6e+05 ... 7.098e+05 7.098e+05array([600015., 600045., 600075., ..., 709725., 709755., 709785.])y(y)float642.4e+06 2.4e+06 ... 2.29e+06array([2399985., 2399955., 2399925., ..., 2290275., 2290245., 2290215.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not specified (based on WGS 84 spheroid)projected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :600000.0 30.0 0.0 2400000.0 0.0 -30.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([600015.0, 600045.0, 600075.0, 600105.0, 600135.0, 600165.0, 600195.0,\n       600225.0, 600255.0, 600285.0,\n       ...\n       709515.0, 709545.0, 709575.0, 709605.0, 709635.0, 709665.0, 709695.0,\n       709725.0, 709755.0, 709785.0],\n      dtype='float64', name='x', length=3660))yPandasIndexPandasIndex(Index([2399985.0, 2399955.0, 2399925.0, 2399895.0, 2399865.0, 2399835.0,\n       2399805.0, 2399775.0, 2399745.0, 2399715.0,\n       ...\n       2290485.0, 2290455.0, 2290425.0, 2290395.0, 2290365.0, 2290335.0,\n       2290305.0, 2290275.0, 2290245.0, 2290215.0],\n      dtype='float64', name='y', length=3660))Attributes: (36)ACCODE :Lasrc; Lasrcadd_offset :0.0AREA_OR_POINT :Areaarop_ave_xshift(meters) :0, 0arop_ave_yshift(meters) :0, 0arop_ncp :0, 0arop_rmse(meters) :0, 0arop_s2_refimg :NONEcloud_coverage :14HLS_PROCESSING_TIME :2022-03-28T18:44:47ZHORIZONTAL_CS_NAME :UTM, WGS84, UTM ZONE 13; UTM, WGS84, UTM ZONE 13L1_PROCESSING_TIME :2020-09-12T20:22:38Z; 2020-09-12T20:38:53ZLANDSAT_PRODUCT_ID :LC08_L1TP_029045_20130610_20200912_02_T1; LC08_L1TP_029046_20130610_20200912_02_T1LANDSAT_SCENE_ID :LC80290452013161LGN02; LC80290462013161LGN02long_name :RedMEAN_SUN_AZIMUTH_ANGLE :80.7093371815872MEAN_SUN_ZENITH_ANGLE :21.7385427039501MEAN_VIEW_AZIMUTH_ANGLE :135.04059864629MEAN_VIEW_ZENITH_ANGLE :3.32796752185848NBAR_SOLAR_ZENITH :19.9568287674254NCOLS :3660NROWS :3660OVR_RESAMPLING_ALG :NEARESTPROCESSING_LEVEL :L1TP; L1TPscale_factor :0.0001SENSING_TIME :2013-06-10T17:19:45.9657940Z; 2013-06-10T17:20:09.8610670ZSENSOR :OLI_TIRS; OLI_TIRSSENTINEL2_TILEID :13QFDspatial_coverage :100SPATIAL_RESOLUTION :30TIRS_SSM_MODEL :ACTUAL; ACTUALTIRS_SSM_POSITION_STATUS :NOMINAL; NOMINALULX :600000ULY :2400000USGS_SOFTWARE :LPGS_15.3.1c_FillValue :-9999\n\n\n\nds.clip()\n\n&lt;matplotlib.collections.QuadMesh at 0x7f53d7a7a5e0&gt;\n\n\n\n\n\n\ngeometries = [\n    {\n        'type': 'Polygon',\n        'coordinates':[geom[\"polygon\"]]\n    }\n]\nclipped = ds.rio.clip(geometries, drop=True, crs=4326)\nclipped\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 71, x: 570)&gt;\narray([[[-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        [-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        [-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        ...,\n        [-9999,   796,   796, ...,  1016,   792, -9999],\n        [  719,   794,   815, ...,   861,   760,   942],\n        [  617,   710,   785, ...,   798,   842,  1165]]], dtype=int16)\nCoordinates:\n  * band         (band) int64 1\n  * x            (x) float64 6.404e+05 6.405e+05 ... 6.575e+05 6.575e+05\n  * y            (y) float64 2.292e+06 2.292e+06 2.292e+06 ... 2.29e+06 2.29e+06\n    spatial_ref  int64 0\nAttributes: (12/36)\n    ACCODE:                    Lasrc; Lasrc\n    add_offset:                0.0\n    AREA_OR_POINT:             Area\n    arop_ave_xshift(meters):   0, 0\n    arop_ave_yshift(meters):   0, 0\n    arop_ncp:                  0, 0\n    ...                        ...\n    TIRS_SSM_MODEL:            ACTUAL; ACTUAL\n    TIRS_SSM_POSITION_STATUS:  NOMINAL; NOMINAL\n    ULX:                       600000\n    ULY:                       2400000\n    USGS_SOFTWARE:             LPGS_15.3.1c\n    _FillValue:                -9999xarray.DataArrayband: 1y: 71x: 570-9999 -9999 -9999 -9999 -9999 -9999 -9999 ... 846 999 971 798 842 1165array([[[-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        [-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        [-9999, -9999, -9999, ..., -9999, -9999, -9999],\n        ...,\n        [-9999,   796,   796, ...,  1016,   792, -9999],\n        [  719,   794,   815, ...,   861,   760,   942],\n        [  617,   710,   785, ...,   798,   842,  1165]]], dtype=int16)Coordinates: (4)band(band)int641array([1])x(x)float646.404e+05 6.405e+05 ... 6.575e+05axis :Xlong_name :x coordinate of projectionstandard_name :projection_x_coordinateunits :metrearray([640425., 640455., 640485., ..., 657435., 657465., 657495.])y(y)float642.292e+06 2.292e+06 ... 2.29e+06axis :Ylong_name :y coordinate of projectionstandard_name :projection_y_coordinateunits :metrearray([2292315., 2292285., 2292255., 2292225., 2292195., 2292165., 2292135.,\n       2292105., 2292075., 2292045., 2292015., 2291985., 2291955., 2291925.,\n       2291895., 2291865., 2291835., 2291805., 2291775., 2291745., 2291715.,\n       2291685., 2291655., 2291625., 2291595., 2291565., 2291535., 2291505.,\n       2291475., 2291445., 2291415., 2291385., 2291355., 2291325., 2291295.,\n       2291265., 2291235., 2291205., 2291175., 2291145., 2291115., 2291085.,\n       2291055., 2291025., 2290995., 2290965., 2290935., 2290905., 2290875.,\n       2290845., 2290815., 2290785., 2290755., 2290725., 2290695., 2290665.,\n       2290635., 2290605., 2290575., 2290545., 2290515., 2290485., 2290455.,\n       2290425., 2290395., 2290365., 2290335., 2290305., 2290275., 2290245.,\n       2290215.])spatial_ref()int640crs_wkt :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :Unknown datum based upon the WGS 84 ellipsoidhorizontal_datum_name :Not specified (based on WGS 84 spheroid)projected_crs_name :UTM Zone 13, Northern Hemispheregrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-105.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"UTM Zone 13, Northern Hemisphere\",GEOGCS[\"Unknown datum based upon the WGS 84 ellipsoid\",DATUM[\"Not specified (based on WGS 84 spheroid)\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-105],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH]]GeoTransform :640410.0 30.0 0.0 2292330.0 0.0 -30.0array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([640425.0, 640455.0, 640485.0, 640515.0, 640545.0, 640575.0, 640605.0,\n       640635.0, 640665.0, 640695.0,\n       ...\n       657225.0, 657255.0, 657285.0, 657315.0, 657345.0, 657375.0, 657405.0,\n       657435.0, 657465.0, 657495.0],\n      dtype='float64', name='x', length=570))yPandasIndexPandasIndex(Index([2292315.0, 2292285.0, 2292255.0, 2292225.0, 2292195.0, 2292165.0,\n       2292135.0, 2292105.0, 2292075.0, 2292045.0, 2292015.0, 2291985.0,\n       2291955.0, 2291925.0, 2291895.0, 2291865.0, 2291835.0, 2291805.0,\n       2291775.0, 2291745.0, 2291715.0, 2291685.0, 2291655.0, 2291625.0,\n       2291595.0, 2291565.0, 2291535.0, 2291505.0, 2291475.0, 2291445.0,\n       2291415.0, 2291385.0, 2291355.0, 2291325.0, 2291295.0, 2291265.0,\n       2291235.0, 2291205.0, 2291175.0, 2291145.0, 2291115.0, 2291085.0,\n       2291055.0, 2291025.0, 2290995.0, 2290965.0, 2290935.0, 2290905.0,\n       2290875.0, 2290845.0, 2290815.0, 2290785.0, 2290755.0, 2290725.0,\n       2290695.0, 2290665.0, 2290635.0, 2290605.0, 2290575.0, 2290545.0,\n       2290515.0, 2290485.0, 2290455.0, 2290425.0, 2290395.0, 2290365.0,\n       2290335.0, 2290305.0, 2290275.0, 2290245.0, 2290215.0],\n      dtype='float64', name='y'))Attributes: (36)ACCODE :Lasrc; Lasrcadd_offset :0.0AREA_OR_POINT :Areaarop_ave_xshift(meters) :0, 0arop_ave_yshift(meters) :0, 0arop_ncp :0, 0arop_rmse(meters) :0, 0arop_s2_refimg :NONEcloud_coverage :14HLS_PROCESSING_TIME :2022-03-28T18:44:47ZHORIZONTAL_CS_NAME :UTM, WGS84, UTM ZONE 13; UTM, WGS84, UTM ZONE 13L1_PROCESSING_TIME :2020-09-12T20:22:38Z; 2020-09-12T20:38:53ZLANDSAT_PRODUCT_ID :LC08_L1TP_029045_20130610_20200912_02_T1; LC08_L1TP_029046_20130610_20200912_02_T1LANDSAT_SCENE_ID :LC80290452013161LGN02; LC80290462013161LGN02long_name :RedMEAN_SUN_AZIMUTH_ANGLE :80.7093371815872MEAN_SUN_ZENITH_ANGLE :21.7385427039501MEAN_VIEW_AZIMUTH_ANGLE :135.04059864629MEAN_VIEW_ZENITH_ANGLE :3.32796752185848NBAR_SOLAR_ZENITH :19.9568287674254NCOLS :3660NROWS :3660OVR_RESAMPLING_ALG :NEARESTPROCESSING_LEVEL :L1TP; L1TPscale_factor :0.0001SENSING_TIME :2013-06-10T17:19:45.9657940Z; 2013-06-10T17:20:09.8610670ZSENSOR :OLI_TIRS; OLI_TIRSSENTINEL2_TILEID :13QFDspatial_coverage :100SPATIAL_RESOLUTION :30TIRS_SSM_MODEL :ACTUAL; ACTUALTIRS_SSM_POSITION_STATUS :NOMINAL; NOMINALULX :600000ULY :2400000USGS_SOFTWARE :LPGS_15.3.1c_FillValue :-9999\n\n\n\nclipped.plot()\n\n&lt;matplotlib.collections.QuadMesh at 0x7f5395c88580&gt;\n\n\n\n\n\n\n\nclipped.hvplot(x=\"x\", y=\"y\", crs=xds.rio.estimate_utm_crs()) * map"
  },
  {
    "objectID": "notebooks/explore.html#next-step-subsetting-in-the-cloud",
    "href": "notebooks/explore.html#next-step-subsetting-in-the-cloud",
    "title": "earthaccess a NASA Earthdata API Client 🌍 in Python",
    "section": "Next Step: Subsetting in the Cloud",
    "text": "Next Step: Subsetting in the Cloud\n\n%%time\n# accessing the data on prem means downloading it if we are in a local environment or \"uploading them\" if we are in the cloud.\norder = earthaccess.subset(results, roi=polygon)\n\n\nRelated links\nCMR API documentation: https://cmr.earthaccess.nasa.gov/search/site/docs/search/api.html\nEDL API documentation: https://urs.earthaccess.nasa.gov/\nNASA OpenScapes: https://nasa-openscapes.github.io/earthaccess-cloud-cookbook/\nNSIDC: https://nsidc.org"
  },
  {
    "objectID": "notebooks/getting-started.html",
    "href": "notebooks/getting-started.html",
    "title": "Getting Started with earthaccess",
    "section": "",
    "text": "Outline notes:\n\nThree lines of code to get the data\nMore details on spatial search options, temporal search, keyword search\n\nEmphasize reproducibility\nearthaccess as a complement to other search tools like Earthdata Search\n\nShow both download to local machine and s3 access: Same code\nComing features / roadmap\n\nearthaccess.explore()\n\nEven though .explore() won’t be fully merged, it’s still good to show upcoming features\n\nKerchunk\n\nCall to action\n\nMake it clear that this is a community effort; highlight what’s already been done"
  },
  {
    "objectID": "notebooks/custom-maps.html",
    "href": "notebooks/custom-maps.html",
    "title": "earthaccess gallery",
    "section": "",
    "text": "import earthaccess\nimport ipyleaflet\nfrom ipywidgets import Dropdown\nfrom shapely.geometry.polygon import orient\nfrom shapely.geometry import Polygon\n\nauth = earthaccess.login()\n\n\n# ArcticDEM\n# note that we need to use the same projection for the image service layer and the map.\nm1 = ipyleaflet.Map(\n    center=(90, 0),\n    zoom=4,\n    basemap=ipyleaflet.basemaps.Esri.ArcticOceanBase,\n    crs=ipyleaflet.projections.EPSG5936.ESRIBasemap,\n)\n# add arctic ocean reference basemap\ntl1 = ipyleaflet.basemap_to_tiles(ipyleaflet.basemaps.Esri.ArcticOceanReference)\nm1.add(tl1)\n\n# create a widget control for the raster function\nraster_functions = [\n    \"Aspect Map\",\n    \"Contour 25\",\n    \"Hillshade Elevation Tinted\",\n    \"Hillshade Gray\",\n    \"Height Ellipsoidal\",\n    \"Height Orthometric\",\n    \"Slope Map\"]\nraster_dropdown1 = Dropdown(\n    value=raster_functions[3],\n    options=raster_functions,\n    description=\"Raster:\",\n)\n\n# add image service layer with ArcticDEM\nurl = 'https://elevation2.arcgis.com/arcgis/rest/services/Polar/ArcticDEM/ImageServer'\nrendering_rule = {\"rasterFunction\": raster_dropdown1.value}\nimage_layer = ipyleaflet.ImageService(\n    name=\"CustomBaseLayer\",\n    url=url,\n    format='jpgpng', rendering_rule=rendering_rule,\n    attribution='Esri, PGC, UMN, NSF, NGA, DigitalGlobe',\n    crs=ipyleaflet.projections.EPSG5936.ESRIBasemap)\n\nm1.add(image_layer) \n\n# add control for raster function\ndropdown_control = ipyleaflet.WidgetControl(widget=raster_dropdown1, position=\"topright\")\nm1.add(dropdown_control)\n\n# set the rendering rule\ndef set_raster_function(sender):\n    image_layer.rendering_rule = {\"rasterFunction\": raster_dropdown1.value}\n    # force redrawing of map by removing and adding layer\n    m1.remove(image_layer)\n    m1.add(image_layer)\n\n\n# watch raster function widget for changes\nraster_dropdown1.observe(set_raster_function)\n\n\n# polygon = [(-83.837578, 45.82529), (-80.167794, 45.82529), (-80.167794, 46.021921), (-83.837578, 46.021921), (-83.837578, 45.82529)]\ncoords = [(-139.59, 59.53), (-138.80, 59.90), (-137.15, 59.45),(-137.30, 58.78),(-138.64, 59.18),(-139.59, 59.53)]\npolygon = list(orient(Polygon(coords)).exterior.coords)\n\n\nif 'sw' in vars():\n    if len(sw.roi) &gt; 0:\n        polygon = sw.roi\nelse:\n    sw = earthaccess.SearchWidget(map=m1)\n\n\nparams = {\n    \"concept_id\" : [\"C2596864127-NSIDC_CPRD\", \"C2076090826-LPCLOUD\", \"C2237824918-ORNL_CLOUD\"],\n    \"temporal\": (\"2020-01\", \"2023-09\"),\n    # \"cloud_cover\": (0, 20),\n    \"polygon\": polygon\n}\nresults = earthaccess.search_data(**params)\nm = sw.explore(results, roi={\"polygon\": polygon})\n# explore will inject its own controls so we add the base layer dropdown back\ntry:\n    sw.map.add(dropdown_control)\nexcept Exception:\n    pass\nm\n\nGranules found: 226"
  },
  {
    "objectID": "notebooks/getting-started.html#analyzing-sea-level-rise",
    "href": "notebooks/getting-started.html#analyzing-sea-level-rise",
    "title": "Getting Started with earthaccess",
    "section": "",
    "text": "Outline notes:\n\nThree lines of code to get the data\nMore details on spatial search options, temporal search, keyword search\n\nEmphasize reproducibility\nearthaccess as a complement to other search tools like Earthdata Search\n\nShow both download to local machine and s3 access: Same code\nComing features / roadmap\n\nearthaccess.explore()\n\nEven though .explore() won’t be fully merged, it’s still good to show upcoming features\n\nKerchunk\n\nCall to action\n\nMake it clear that this is a community effort; highlight what’s already been done"
  },
  {
    "objectID": "notebooks/getting-started.html#overview",
    "href": "notebooks/getting-started.html#overview",
    "title": "Getting Started with earthaccess",
    "section": "Overview",
    "text": "Overview\nThis tutorial analyzes global sea level rise from satellite altimetry data and compares it to a historic record. We will be reproducing the plot from this infographic: NASA-led Study Reveals the Causes of Sea Level Rise Since 1900."
  },
  {
    "objectID": "notebooks/getting-started.html#learning-objectives",
    "href": "notebooks/getting-started.html#learning-objectives",
    "title": "Getting Started with earthaccess",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSearch for data programmatically using keywords or datasets’ concept_id\nAccess data using the earthaccess python library\nVisualize sea level rise trends from altimetry datasets and compare against historic records"
  },
  {
    "objectID": "notebooks/getting-started.html#requirements",
    "href": "notebooks/getting-started.html#requirements",
    "title": "Getting Started with earthaccess",
    "section": "Requirements",
    "text": "Requirements\n\n1. Compute environment\n\nThis notebook can run anywhere thanks to earthaccess!\n\n\n\n2. Earthdata Login\nAn Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Thus, to access NASA data, you need Earthdata Login. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n\n\nImport (or Install) Packages\n\nimport logging\nlogging.basicConfig(level=logging.INFO,\n                    force = True)\n\ntry:\n    import earthaccess\n    import xarray as xr\n    from pyproj import Geod\n    import numpy as np\n    import hvplot.xarray\n    from matplotlib import pyplot as plt\n    from pprint import pprint\n    import panel as pn\n    import panel.widgets as pnw\nexcept ImportError as e:\n    logging.warning(\"installing missing dependencies... \")\n    %pip install -q earthaccess matplotlib hvplot pyproj xarray numpy h5netcdf panel\nfinally:\n    import earthaccess\n    import xarray as xr\n    from pyproj import Geod\n    import numpy as np\n    import hvplot.xarray\n    from matplotlib import pyplot as plt\n    from pprint import pprint\n    import panel.widgets as pnw\n    logging.info(\"Dependencies imported\")\n    \n\n\n\n\n\n\n\n\n\n\n\nINFO:root:Dependencies imported"
  },
  {
    "objectID": "notebooks/getting-started.html#search-for-sea-surface-height-data",
    "href": "notebooks/getting-started.html#search-for-sea-surface-height-data",
    "title": "Getting Started with earthaccess",
    "section": "Search for Sea Surface Height Data",
    "text": "Search for Sea Surface Height Data\nLet’s find the first four collections that match a keyword search for Sea Surface Height and print out the first two.\n\nmax_results = 10\n\ncollections = earthaccess.search_datasets(\n    keyword = \"SEA SURFACE HEIGHT\",\n    cloud_hosted = True,\n    count = max_results\n)\n\n# Let's print 2 collections\nfor collection in collections[0:2]:\n    # pprint(collection.summary())\n    print(pprint(collection.summary()), collection.abstract(), \"\\n\", collection[\"umm\"][\"DOI\"], \"\\n\\n\")\n\nDatasets found: 245\n{'cloud-info': {'Region': 'us-west-2',\n                'S3BucketAndObjectPrefixNames': ['podaac-ops-cumulus-public/SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205/',\n                                                 'podaac-ops-cumulus-protected/SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205/'],\n                'S3CredentialsAPIDocumentationURL': 'https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME',\n                'S3CredentialsAPIEndpoint': 'https://archive.podaac.earthdata.nasa.gov/s3credentials'},\n 'concept-id': 'C2270392799-POCLOUD',\n 'file-type': \"[{'Format': 'netCDF-4', 'FormatType': 'Native', \"\n              \"'AverageFileSize': 9.7, 'AverageFileSizeUnit': 'MB'}]\",\n 'get-data': ['https://cmr.earthdata.nasa.gov/virtual-directory/collections/C2270392799-POCLOUD',\n              'https://search.earthdata.nasa.gov/search/granules?p=C2270392799-POCLOUD'],\n 'short-name': 'SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205',\n 'version': '2205'}\nNone This dataset provides gridded Sea Surface Height Anomalies (SSHA) above a mean sea surface, on a 1/6th degree grid every 5 days. It contains the fully corrected heights, with a delay of up to 3 months. The gridded data are derived from the along-track SSHA data of TOPEX/Poseidon, Jason-1, Jason-2, Jason-3 and Jason-CS (Sentinel-6) as reference data from the level 2 along-track data found at https://podaac.jpl.nasa.gov/dataset/MERGED_TP_J1_OSTM_OST_CYCLES_V51, plus ERS-1, ERS-2, Envisat, SARAL-AltiKa, CryoSat-2, Sentinel-3A, Sentinel-3B, depending on the date, from the RADS database. The date given in the grid files is the center of the 5-day window. The grids were produced from altimeter data using Kriging interpolation, which gives best linear prediction based upon prior knowledge of covariance. \n {'DOI': '10.5067/SLREF-CDRV3', 'Authority': 'https://doi.org'} \n\n\n{'cloud-info': {'Region': 'us-west-2',\n                'S3BucketAndObjectPrefixNames': ['podaac-ops-cumulus-protected/ALTIKA_SARAL_L2_OST_XOGDR/',\n                                                 'podaac-ops-cumulus-public/ALTIKA_SARAL_L2_OST_XOGDR/'],\n                'S3CredentialsAPIDocumentationURL': 'https://archive.podaac.earthdata.nasa.gov/s3credentialsREADME',\n                'S3CredentialsAPIEndpoint': 'https://archive.podaac.earthdata.nasa.gov/s3credentials'},\n 'concept-id': 'C2251465126-POCLOUD',\n 'file-type': \"[{'Format': 'netCDF-4', 'FormatType': 'Native'}]\",\n 'get-data': ['https://search.earthdata.nasa.gov/search/granules?p=C2251465126-POCLOUD',\n              'https://cmr.earthdata.nasa.gov/virtual-directory/collections/C2251465126-POCLOUD'],\n 'short-name': 'ALTIKA_SARAL_L2_OST_XOGDR',\n 'version': 'f'}\nNone These data are near-real-time (NRT) (within 7-9 hours of measurement) sea surface height anomalies (SSHA) from the AltiKa altimeter onboard the Satellite with ARgos and ALtiKa (SARAL).  SARAL is a French(CNES)/Indian(SARAL) collaborative mission to measure sea surface height using the Ka-band AltiKa altimeter and was launched February 25, 2013. The major difference between these data and the Operational Geophysical Data Record (OGDR) data produced by the project is that the orbit from SARAL has been adjusted using SSHA differences with those from the OSTM/Jason-2 GPS-OGDR-SSHA product at inter-satellite crossover locations. This produces a more accurate NRT orbit altitude for SARAL with accuracy of 1.5 cm (RMS), taking advantage of the 1 cm (radial RMS) accuracy of the GPS-based orbit used for the OSTM/Jason-2 GPS-OGDR-SSHA product. This dataset also contains all data from the project (reduced) OGDR, and improved altimeter wind speeds and sea state bias correction. More information on the SARAL mission can be found at: http://www.aviso.oceanobs.com/en/missions/current-missions/saral.html \n {'DOI': '10.5067/AKASA-XOGD1', 'Authority': 'https://doi.org'}"
  },
  {
    "objectID": "notebooks/getting-started.html#access-data",
    "href": "notebooks/getting-started.html#access-data",
    "title": "Getting Started with earthaccess",
    "section": "Access Data",
    "text": "Access Data\nThe first dataset looks like it contains data from many altimetry missions, let’s explore a bit more! We will access the first granule of the SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205 collection in the month of May for every year that data is available and open the granules via xarray.\n\n\ngranules = []\n\nfor year in range(1990, 2020):\n    print(f\"Searching for data in {year}\")\n    granule = earthaccess.search_data(doi=\"10.5067/SLREF-CDRV3\",\n                                      temporal= (f\"{year}-05\", f\"{year}-06\"),\n                                      count = 1)\n    if len(granule)&gt;0:\n        granules.append(granule[0])\nprint(f\"Total granules: {len(granules)}\")     \n\nSearching for data in 1990\nGranules found: 0\nSearching for data in 1991\nGranules found: 0\nSearching for data in 1992\nGranules found: 0\nSearching for data in 1993\nGranules found: 6\nSearching for data in 1994\nGranules found: 6\nSearching for data in 1995\nGranules found: 6\nSearching for data in 1996\nGranules found: 7\nSearching for data in 1997\nGranules found: 7\nSearching for data in 1998\nGranules found: 7\nSearching for data in 1999\nGranules found: 7\nSearching for data in 2000\nGranules found: 7\nSearching for data in 2001\nGranules found: 7\nSearching for data in 2002\nGranules found: 7\nSearching for data in 2003\nGranules found: 7\nSearching for data in 2004\nGranules found: 6\nSearching for data in 2005\nGranules found: 6\nSearching for data in 2006\nGranules found: 6\nSearching for data in 2007\nGranules found: 6\nSearching for data in 2008\nGranules found: 6\nSearching for data in 2009\nGranules found: 6\nSearching for data in 2010\nGranules found: 6\nSearching for data in 2011\nGranules found: 6\nSearching for data in 2012\nGranules found: 6\nSearching for data in 2013\nGranules found: 6\nSearching for data in 2014\nGranules found: 6\nSearching for data in 2015\nGranules found: 6\nSearching for data in 2016\nGranules found: 7\nSearching for data in 2017\nGranules found: 7\nSearching for data in 2018\nGranules found: 7\nSearching for data in 2019\nGranules found: 7\nTotal granules: 27\n\n\n\n%%time\n\nds_SSH = xr.open_mfdataset(earthaccess.open(granules), chunks={})\nds_SSH\n\nOpening 27 granules, approx size: 0.24 GB\nCPU times: user 4.63 s, sys: 668 ms, total: 5.3 s\nWall time: 1min 55s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:      (Time: 27, Longitude: 2160, nv: 2, Latitude: 960)\nCoordinates:\n  * Longitude    (Longitude) float32 0.08333 0.25 0.4167 ... 359.6 359.8 359.9\n  * Latitude     (Latitude) float32 -79.92 -79.75 -79.58 ... 79.58 79.75 79.92\n  * Time         (Time) datetime64[ns] 1993-05-03T12:00:00 ... 2019-05-02T12:...\nDimensions without coordinates: nv\nData variables:\n    Lon_bounds   (Time, Longitude, nv) float32 dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;\n    Lat_bounds   (Time, Latitude, nv) float32 dask.array&lt;chunksize=(1, 960, 2), meta=np.ndarray&gt;\n    Time_bounds  (Time, nv) datetime64[ns] dask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\n    SLA          (Time, Latitude, Longitude) float32 dask.array&lt;chunksize=(1, 960, 2160), meta=np.ndarray&gt;\n    SLA_ERR      (Time, Latitude, Longitude) float32 dask.array&lt;chunksize=(1, 960, 2160), meta=np.ndarray&gt;\nAttributes: (12/21)\n    Conventions:            CF-1.6\n    ncei_template_version:  NCEI_NetCDF_Grid_Template_v2.0\n    Institution:            Jet Propulsion Laboratory\n    geospatial_lat_min:     -79.916664\n    geospatial_lat_max:     79.916664\n    geospatial_lon_min:     0.083333336\n    ...                     ...\n    version_number:         2205\n    Data_Pnts_Each_Sat:     {\"16\": 780190, \"1001\": 668288}\n    source_version:         commit dc95db885c920084614a41849ce5a7d417198ef3\n    SLA_Global_MEAN:        -0.027657876081274502\n    SLA_Global_STD:         0.0859016072308609\n    latency:                finalxarray.DatasetDimensions:Time: 27Longitude: 2160nv: 2Latitude: 960Coordinates: (3)Longitude(Longitude)float320.08333 0.25 0.4167 ... 359.8 359.9standard_name :longitudeunits :degrees_eastpoint_spacing :evenlong_name :longitudeaxis :Xbounds :Lon_boundsarray([8.333334e-02, 2.500000e-01, 4.166667e-01, ..., 3.595833e+02,\n       3.597500e+02, 3.599167e+02], dtype=float32)Latitude(Latitude)float32-79.92 -79.75 ... 79.75 79.92standard_name :latitudeunits :degrees_northpoint_spacing :evenlong_name :latitudeaxis :Ybounds :Lat_boundsarray([-79.916664, -79.75    , -79.583336, ...,  79.583336,  79.75    ,\n        79.916664], dtype=float32)Time(Time)datetime64[ns]1993-05-03T12:00:00 ... 2019-05-...standard_name :timelong_name :Timebounds :Time_boundsaxis :Tarray(['1993-05-03T12:00:00.000000000', '1994-05-03T12:00:00.000000000',\n       '1995-05-03T12:00:00.000000000', '1996-05-02T12:00:00.000000000',\n       '1997-05-02T12:00:00.000000000', '1998-05-02T12:00:00.000000000',\n       '1999-05-02T12:00:00.000000000', '2000-05-01T12:00:00.000000000',\n       '2001-05-01T12:00:00.000000000', '2002-05-01T12:00:00.000000000',\n       '2003-05-01T12:00:00.000000000', '2004-05-05T12:00:00.000000000',\n       '2005-05-05T12:00:00.000000000', '2006-05-05T12:00:00.000000000',\n       '2007-05-05T12:00:00.000000000', '2008-05-04T12:00:00.000000000',\n       '2009-05-04T12:00:00.000000000', '2010-05-04T12:00:00.000000000',\n       '2011-05-04T12:00:00.000000000', '2012-05-03T12:00:00.000000000',\n       '2013-05-03T12:00:00.000000000', '2014-05-03T12:00:00.000000000',\n       '2015-05-03T12:00:00.000000000', '2016-05-02T12:00:00.000000000',\n       '2017-05-02T12:00:00.000000000', '2018-05-02T12:00:00.000000000',\n       '2019-05-02T12:00:00.000000000'], dtype='datetime64[ns]')Data variables: (5)Lon_bounds(Time, Longitude, nv)float32dask.array&lt;chunksize=(1, 2160, 2), meta=np.ndarray&gt;units :degrees_eastcomment :longitude values at the west and east bounds of each pixel.\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n455.62 kiB\n16.88 kiB\n\n\nShape\n(27, 2160, 2)\n(1, 2160, 2)\n\n\nDask graph\n27 chunks in 82 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nLat_bounds\n\n\n(Time, Latitude, nv)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 960, 2), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\ndegrees_north\n\ncomment :\n\nlatitude values at the north and south bounds of each pixel.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n202.50 kiB\n7.50 kiB\n\n\nShape\n(27, 960, 2)\n(1, 960, 2)\n\n\nDask graph\n27 chunks in 82 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nTime_bounds\n\n\n(Time, nv)\n\n\ndatetime64[ns]\n\n\ndask.array&lt;chunksize=(1, 2), meta=np.ndarray&gt;\n\n\n\n\ncomment :\n\nTime bounds for each time value, same value as time variable. The time variable is defined on points instead of on bounding boxes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n432 B\n16 B\n\n\nShape\n(27, 2)\n(1, 2)\n\n\nDask graph\n27 chunks in 55 graph layers\n\n\nData type\ndatetime64[ns] numpy.ndarray\n\n\n\n\n\n\n\n\n\nSLA\n\n\n(Time, Latitude, Longitude)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 960, 2160), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nm\n\nlong_name :\n\nSea Level Anomaly Estimate\n\nstandard_name :\n\nsea_surface_height_above_sea_level\n\nalias :\n\nsea_surface_height_above_sea_level\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n213.57 MiB\n7.91 MiB\n\n\nShape\n(27, 960, 2160)\n(1, 960, 2160)\n\n\nDask graph\n27 chunks in 55 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nSLA_ERR\n\n\n(Time, Latitude, Longitude)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(1, 960, 2160), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nm\n\nlong_name :\n\nSea Level Anomaly Error Estimate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n213.57 MiB\n7.91 MiB\n\n\nShape\n(27, 960, 2160)\n(1, 960, 2160)\n\n\nDask graph\n27 chunks in 55 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nIndexes: (3)LongitudePandasIndexPandasIndex(Float64Index([0.0833333358168602,               0.25, 0.4166666567325592,\n              0.5833333134651184,               0.75, 0.9166666865348816,\n              1.0833333730697632,               1.25, 1.4166666269302368,\n              1.5833333730697632,\n              ...\n               358.4166564941406,  358.5833435058594,             358.75,\n               358.9166564941406,  359.0833435058594,             359.25,\n               359.4166564941406,  359.5833435058594,             359.75,\n               359.9166564941406],\n             dtype='float64', name='Longitude', length=2160))LatitudePandasIndexPandasIndex(Float64Index([-79.91666412353516,             -79.75, -79.58333587646484,\n              -79.41666412353516,             -79.25, -79.08333587646484,\n              -78.91666412353516,             -78.75, -78.58333587646484,\n              -78.41666412353516,\n              ...\n               78.41666412353516,  78.58333587646484,              78.75,\n               78.91666412353516,  79.08333587646484,              79.25,\n               79.41666412353516,  79.58333587646484,              79.75,\n               79.91666412353516],\n             dtype='float64', name='Latitude', length=960))TimePandasIndexPandasIndex(DatetimeIndex(['1993-05-03 12:00:00', '1994-05-03 12:00:00',\n               '1995-05-03 12:00:00', '1996-05-02 12:00:00',\n               '1997-05-02 12:00:00', '1998-05-02 12:00:00',\n               '1999-05-02 12:00:00', '2000-05-01 12:00:00',\n               '2001-05-01 12:00:00', '2002-05-01 12:00:00',\n               '2003-05-01 12:00:00', '2004-05-05 12:00:00',\n               '2005-05-05 12:00:00', '2006-05-05 12:00:00',\n               '2007-05-05 12:00:00', '2008-05-04 12:00:00',\n               '2009-05-04 12:00:00', '2010-05-04 12:00:00',\n               '2011-05-04 12:00:00', '2012-05-03 12:00:00',\n               '2013-05-03 12:00:00', '2014-05-03 12:00:00',\n               '2015-05-03 12:00:00', '2016-05-02 12:00:00',\n               '2017-05-02 12:00:00', '2018-05-02 12:00:00',\n               '2019-05-02 12:00:00'],\n              dtype='datetime64[ns]', name='Time', freq=None))Attributes: (21)Conventions :CF-1.6ncei_template_version :NCEI_NetCDF_Grid_Template_v2.0Institution :Jet Propulsion Laboratorygeospatial_lat_min :-79.916664geospatial_lat_max :79.916664geospatial_lon_min :0.083333336geospatial_lon_max :359.91666time_coverage_start :1993-05-03time_coverage_end :1993-05-03date_created :2022-10-30T20:40:50.716092title :Sea Level Anomaly Estimate based on Altimeter Data, final product (replaced interim version).short_name :SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL2205long_name :MEaSUREs Gridded Sea Surface Height Anomalies Version 2205summary :Sea level anomaly grids from altimeter data using Kriging interpolation, which gives best linear prediction based upon prior knowledge of covariance. DOI :10.5067/SLREF-CDRV3version_number :2205Data_Pnts_Each_Sat :{\"16\": 780190, \"1001\": 668288}source_version :commit dc95db885c920084614a41849ce5a7d417198ef3SLA_Global_MEAN :-0.027657876081274502SLA_Global_STD :0.0859016072308609latency :final"
  },
  {
    "objectID": "notebooks/getting-started.html#plot-sea-level-anomalies",
    "href": "notebooks/getting-started.html#plot-sea-level-anomalies",
    "title": "Getting Started with earthaccess",
    "section": "Plot Sea Level Anomalies",
    "text": "Plot Sea Level Anomalies\n\n\ntime = pnw.Player(name='Time', start=0, end=len(ds_SSH.Time)-1, loop_policy='loop', interval=1000)\n\nds_SSH.SLA.interactive(loc='bottom', aspect=\"equal\").isel(Time=time).hvplot(cmap=\"inferno\", data_aspect=1)"
  },
  {
    "objectID": "notebooks/getting-started.html#recreate-the-sea-level-rise-infographic",
    "href": "notebooks/getting-started.html#recreate-the-sea-level-rise-infographic",
    "title": "Getting Started with earthaccess",
    "section": "Recreate the Sea Level Rise Infographic",
    "text": "Recreate the Sea Level Rise Infographic\nFirst, we define a function that will calculate the area in 1/6 by 1/6 degree boxes in order to calculate the global mean of the SSH later.\n\ndef ssl_area(lats):\n    \"\"\"\n    Calculate the area associated with a 1/6 by 1/6 degree box at latitude specified in 'lats'.\n    \n    Parameter\n    ==========\n    lats: a list or numpy array of size N the latitudes of interest. \n    \n    Return\n    =======\n    out: Array (N) area values (unit: m^2)\n    \"\"\"\n    # Define WGS84 as CRS:\n    geod = Geod(ellps='WGS84')\n    dx=1/12.0\n    # create a lambda function for calculating the perimeters of the boxes\n    c_area=lambda lat: geod.polygon_area_perimeter(np.r_[-dx,dx,dx,-dx], lat+np.r_[-dx,-dx,dx,dx])[0]\n    out=[]\n    for lat in lats:\n        out.append(c_area(lat))\n    return np.array(out)\n\nLet’s use the function on our Sea Surface Height dataset.\n\n# note: they rotated the data in the last release, this operation used to be (1,-1)\nssh_area = ssl_area(ds_SSH.Latitude.data).reshape(-1,1)\nprint(ssh_area.shape)\n\n(960, 1)\n\n\nNext, we find and open the historic record dataset also using earthaccess and xarray.\n\nhistoric_ts_results = earthaccess.search_data(short_name='JPL_RECON_GMSL')\nhistoric_ts=xr.open_mfdataset(earthaccess.open([historic_ts_results[0]]), engine='h5netcdf')\nhistoric_ts\n\nGranules found: 1\nOpening 1 granules, approx size: 0.0 GB\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                                             (time: 119)\nCoordinates:\n  * time                                                (time) datetime64[ns] ...\nData variables: (12/21)\n    global_average_sea_level_change                     (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n    global_average_sea_level_change_upper               (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n    global_average_sea_level_change_lower               (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n    glac_mean                                           (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n    glac_upper                                          (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n    glac_lower                                          (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n    ...                                                  ...\n    global_average_thermosteric_sea_level_change        (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n    global_average_thermosteric_sea_level_change_upper  (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n    global_average_thermosteric_sea_level_change_lower  (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n    sum_of_contrib_processes_mean                       (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n    sum_of_contrib_processes_upper                      (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n    sum_of_contrib_processes_lower                      (time) float32 dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\nAttributes: (12/42)\n    title:                     Global sea-level changes and contributing proc...\n    summary:                   This file contains reconstructed global-mean s...\n    id:                        10.5067/GMSLT-FJPL1\n    naming_authority:          gov.nasa.jpl\n    source:                    Frederikse et al. The causes of sea-level rise...\n    project:                   NASA sea-level change science team (N-SLCT)\n    ...                        ...\n    time_coverage_start:       1900-01-01\n    time_coverage_end:         2018-12-31\n    time_coverage_duration:    P119Y\n    time_coverage_resolution:  P1Y\n    date_created:              2020-07-28\n    date_modified:             2020-09-14xarray.DatasetDimensions:time: 119Coordinates: (1)time(time)datetime64[ns]1900-06-15 ... 2018-06-15long_name :timestandard_name :timearray(['1900-06-15T00:00:00.000000000', '1901-06-15T00:00:00.000000000',\n       '1902-06-15T00:00:00.000000000', '1903-06-15T00:00:00.000000000',\n       '1904-06-15T00:00:00.000000000', '1905-06-15T00:00:00.000000000',\n       '1906-06-15T00:00:00.000000000', '1907-06-15T00:00:00.000000000',\n       '1908-06-15T00:00:00.000000000', '1909-06-15T00:00:00.000000000',\n       '1910-06-15T00:00:00.000000000', '1911-06-15T00:00:00.000000000',\n       '1912-06-15T00:00:00.000000000', '1913-06-15T00:00:00.000000000',\n       '1914-06-15T00:00:00.000000000', '1915-06-15T00:00:00.000000000',\n       '1916-06-15T00:00:00.000000000', '1917-06-15T00:00:00.000000000',\n       '1918-06-15T00:00:00.000000000', '1919-06-15T00:00:00.000000000',\n       '1920-06-15T00:00:00.000000000', '1921-06-15T00:00:00.000000000',\n       '1922-06-15T00:00:00.000000000', '1923-06-15T00:00:00.000000000',\n       '1924-06-15T00:00:00.000000000', '1925-06-15T00:00:00.000000000',\n       '1926-06-15T00:00:00.000000000', '1927-06-15T00:00:00.000000000',\n       '1928-06-15T00:00:00.000000000', '1929-06-15T00:00:00.000000000',\n       '1930-06-15T00:00:00.000000000', '1931-06-15T00:00:00.000000000',\n       '1932-06-15T00:00:00.000000000', '1933-06-15T00:00:00.000000000',\n       '1934-06-15T00:00:00.000000000', '1935-06-15T00:00:00.000000000',\n       '1936-06-15T00:00:00.000000000', '1937-06-15T00:00:00.000000000',\n       '1938-06-15T00:00:00.000000000', '1939-06-15T00:00:00.000000000',\n       '1940-06-15T00:00:00.000000000', '1941-06-15T00:00:00.000000000',\n       '1942-06-15T00:00:00.000000000', '1943-06-15T00:00:00.000000000',\n       '1944-06-15T00:00:00.000000000', '1945-06-15T00:00:00.000000000',\n       '1946-06-15T00:00:00.000000000', '1947-06-15T00:00:00.000000000',\n       '1948-06-15T00:00:00.000000000', '1949-06-15T00:00:00.000000000',\n       '1950-06-15T00:00:00.000000000', '1951-06-15T00:00:00.000000000',\n       '1952-06-15T00:00:00.000000000', '1953-06-15T00:00:00.000000000',\n       '1954-06-15T00:00:00.000000000', '1955-06-15T00:00:00.000000000',\n       '1956-06-15T00:00:00.000000000', '1957-06-15T00:00:00.000000000',\n       '1958-06-15T00:00:00.000000000', '1959-06-15T00:00:00.000000000',\n       '1960-06-15T00:00:00.000000000', '1961-06-15T00:00:00.000000000',\n       '1962-06-15T00:00:00.000000000', '1963-06-15T00:00:00.000000000',\n       '1964-06-15T00:00:00.000000000', '1965-06-15T00:00:00.000000000',\n       '1966-06-15T00:00:00.000000000', '1967-06-15T00:00:00.000000000',\n       '1968-06-15T00:00:00.000000000', '1969-06-15T00:00:00.000000000',\n       '1970-06-15T00:00:00.000000000', '1971-06-15T00:00:00.000000000',\n       '1972-06-15T00:00:00.000000000', '1973-06-15T00:00:00.000000000',\n       '1974-06-15T00:00:00.000000000', '1975-06-15T00:00:00.000000000',\n       '1976-06-15T00:00:00.000000000', '1977-06-15T00:00:00.000000000',\n       '1978-06-15T00:00:00.000000000', '1979-06-15T00:00:00.000000000',\n       '1980-06-15T00:00:00.000000000', '1981-06-15T00:00:00.000000000',\n       '1982-06-15T00:00:00.000000000', '1983-06-15T00:00:00.000000000',\n       '1984-06-15T00:00:00.000000000', '1985-06-15T00:00:00.000000000',\n       '1986-06-15T00:00:00.000000000', '1987-06-15T00:00:00.000000000',\n       '1988-06-15T00:00:00.000000000', '1989-06-15T00:00:00.000000000',\n       '1990-06-15T00:00:00.000000000', '1991-06-15T00:00:00.000000000',\n       '1992-06-15T00:00:00.000000000', '1993-06-15T00:00:00.000000000',\n       '1994-06-15T00:00:00.000000000', '1995-06-15T00:00:00.000000000',\n       '1996-06-15T00:00:00.000000000', '1997-06-15T00:00:00.000000000',\n       '1998-06-15T00:00:00.000000000', '1999-06-15T00:00:00.000000000',\n       '2000-06-15T00:00:00.000000000', '2001-06-15T00:00:00.000000000',\n       '2002-06-15T00:00:00.000000000', '2003-06-15T00:00:00.000000000',\n       '2004-06-15T00:00:00.000000000', '2005-06-15T00:00:00.000000000',\n       '2006-06-15T00:00:00.000000000', '2007-06-15T00:00:00.000000000',\n       '2008-06-15T00:00:00.000000000', '2009-06-15T00:00:00.000000000',\n       '2010-06-15T00:00:00.000000000', '2011-06-15T00:00:00.000000000',\n       '2012-06-15T00:00:00.000000000', '2013-06-15T00:00:00.000000000',\n       '2014-06-15T00:00:00.000000000', '2015-06-15T00:00:00.000000000',\n       '2016-06-15T00:00:00.000000000', '2017-06-15T00:00:00.000000000',\n       '2018-06-15T00:00:00.000000000'], dtype='datetime64[ns]')Data variables: (21)global_average_sea_level_change(time)float32dask.array&lt;chunksize=(119,), meta=np.ndarray&gt;units :mmlong_name :Observed global-average sea level (mean value)valid_min :-1000000.0valid_max :1000000.0comment :Value is relative to 2002-2019 baselinestandard_name :global_average_sea_level_changecoverage_content_type :physicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\nglobal_average_sea_level_change_upper\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nObserved global-average sea level (upper bound)\n\nstandard_name :\n\nglobal_average_sea_level_change\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nglobal_average_sea_level_change_lower\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nObserved global-mean sea level (lower bound)\n\nstandard_name :\n\nglobal_average_sea_level_change\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nglac_mean\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nGlacier contribution (mean value)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline. The glacier term does not include peripheral glaciers of both ice sheets\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nglac_upper\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nGlacier contribution (upper bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline. The glacier term does not include peripheral glaciers of both ice sheets\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nglac_lower\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nGlacier contribution (lower bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline. The glacier term does not include peripheral glaciers of both ice sheets\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nGrIS_mean\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nGreenland Ice Sheet contribution (mean value)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline. This term includes glaciers in the Greenland periphery\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nGrIS_upper\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nGreenland Ice Sheet contribution (upper bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline. This term includes glaciers in the Greenland periphery\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nGrIS_lower\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nGreenland Ice Sheet contribution (lower bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline. This term includes glaciers in the Greenland periphery\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nAIS_mean\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nAntarctic Ice Sheet contribution (mean value)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline.\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nAIS_upper\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nAntarctic Ice Sheet contribution (upper bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline.\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nAIS_lower\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nAntarctic Ice Sheet contribution (lower bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline.\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntws_mean\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nTerrestrial water storage contribution (mean value)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline. This term includes the effects for groundwater depletion, reservoir impoundment and non-anthropogenic tws changes\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntws_upper\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nTerrestrial water storage contribution (upper bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline. This term includes the effects for groundwater depletion, reservoir impoundment and non-anthropogenic tws changes\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\ntws_lower\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nTerrestrial water storage contribution (lower bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline. This term includes the effects for groundwater depletion, reservoir impoundment and non-anthropogenic tws changes\n\nstandard_name :\n\nglobal_average_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nglobal_average_thermosteric_sea_level_change\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nGlobal thermosteric contribution (mean value)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline\n\nstandard_name :\n\nglobal_average_thermosteric_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nglobal_average_thermosteric_sea_level_change_upper\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nGlobal thermosteric contribution (upper bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline\n\nstandard_name :\n\nglobal_average_thermosteric_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nglobal_average_thermosteric_sea_level_change_lower\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nGlobal thermosteric contribution (lower bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline\n\nstandard_name :\n\nglobal_average_thermosteric_sea_level_change\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsum_of_contrib_processes_mean\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nSum of contributing processes (mean value)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsum_of_contrib_processes_upper\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nSum of contributing processes (upper bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nsum_of_contrib_processes_lower\n\n\n(time)\n\n\nfloat32\n\n\ndask.array&lt;chunksize=(119,), meta=np.ndarray&gt;\n\n\n\n\nunits :\n\nmm\n\nlong_name :\n\nSum of contributing processes (lower bound)\n\nvalid_min :\n\n-1000000.0\n\nvalid_max :\n\n1000000.0\n\ncomment :\n\nValue is relative to 2002-2019 baseline\n\ncoverage_content_type :\n\nphysicalMeasurement\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n476 B\n476 B\n\n\nShape\n(119,)\n(119,)\n\n\nDask graph\n1 chunks in 2 graph layers\n\n\nData type\nfloat32 numpy.ndarray\n\n\n\n\n\n\n\n\n\nIndexes: (1)timePandasIndexPandasIndex(DatetimeIndex(['1900-06-15', '1901-06-15', '1902-06-15', '1903-06-15',\n               '1904-06-15', '1905-06-15', '1906-06-15', '1907-06-15',\n               '1908-06-15', '1909-06-15',\n               ...\n               '2009-06-15', '2010-06-15', '2011-06-15', '2012-06-15',\n               '2013-06-15', '2014-06-15', '2015-06-15', '2016-06-15',\n               '2017-06-15', '2018-06-15'],\n              dtype='datetime64[ns]', name='time', length=119, freq=None))Attributes: (42)title :Global sea-level changes and contributing processes over 1900-2018summary :This file contains reconstructed global-mean sea level evolution and the estimated contributing processes over 1900-2018. Reconstructed sea level is based on annual-mean tide-gauge observations and uses the virtual-station method to aggregrate the individual observations into a global estimate. The contributing processes consist of thermosteric changes, glacier mass changes, mass changes of the Greenland and Antarctic Ice Sheet, and terrestrial water storage changes. The glacier, ice sheet, and terrestrial water storage are estimated by combining GRACE observations (2003-2018) with long-term estimates from in-situ observations and models. Steric estimates are based on in-situ temperature profiles. The upper- and lower bound represent the 5 and 95 percent confidence level. The numbers are equal to the ones presented in Frederikse et al. The causes of sea-level rise since 1900, Nature, 2020, reformatted to meet the specifications of the JPL PO.DAACid :10.5067/GMSLT-FJPL1naming_authority :gov.nasa.jplsource :Frederikse et al. The causes of sea-level rise since 1900, Nature, 2020 https://doi.org/10.1038/s41586-020-2591-3project :NASA sea-level change science team (N-SLCT)program :NASA sea-level change science team (N-SLCT)institution :NASA Jet Propulsion Laboratory (JPL)references :https://doi.org/10.5067/GMSLT-FJPL1,https://doi.org/10.1038/s41586-020-2591-3acknowledgement :This research was carried out by the Jet Propulsion Laboratory, managed by the California Institute of Technology under a contract with the National Aeronautics and Space Administration.processing_level :4product_version :1.0license :Public Domainhistory :This version provides the data as presented in Frederikse et al. 2020.Conventions :CF-1.7,ACDD-1.3keywords :EARTH SCIENCE &gt; CLIMATE INDICATORS &gt; ATMOSPHERIC/OCEAN INDICATORS &gt; SEA LEVEL RISEkeywords_vocabulary :GCMD Science Keywordsstandard_name_vocabulary :CF Standard Name Table v27creator_name :Thomas Frederiksecreator_type :personcreator_url :https://sealevel.nasa.govcreator_email :thomas.frederikse@jpl.nasa.govcreator_institution :NASA Jet Propulsion Laboratory (JPL)contributor_name :Thomas Frederikse, Felix Landerer, Lambert Caron, Surendra Adhikari, David Parkes, Vincent Humphrey, Sönke Dangendorf, Peter Hogarth, Laure Zanna, Lijing Cheng, Yun-Hao Wucontributor_role :main author,author,author,author,author,author,author,author,author,author,authorpublisher_name :Physical Oceanography Distributed Active Archive Center (PO.DAAC)publisher_type :grouppublisher_url :https://podaac.jpl.nasa.govpublisher_email :podaac@podaac.jpl.nasa.govpublisher_institution :NASA Jet Propulsion Laboratory (JPL)geospatial_lat_min :-90.0geospatial_lat_max :90.0geospatial_lat_units :degrees_northgeospatial_lon_min :-180.0geospatial_lon_max :180.0geospatial_lon_units :degrees_easttime_coverage_start :1900-01-01time_coverage_end :2018-12-31time_coverage_duration :P119Ytime_coverage_resolution :P1Ydate_created :2020-07-28date_modified :2020-09-14"
  },
  {
    "objectID": "notebooks/getting-started.html#earthaccess",
    "href": "notebooks/getting-started.html#earthaccess",
    "title": "Getting Started with earthaccess",
    "section": "earthaccess",
    "text": "earthaccess\nWe recommend authenticating your Earthdata Login (EDL) information using the earthaccess python package as follows:\n\nauth = earthaccess.login()"
  },
  {
    "objectID": "notebooks/getting-started.html#earthaccess-and-nasas-edl",
    "href": "notebooks/getting-started.html#earthaccess-and-nasas-edl",
    "title": "Getting Started with earthaccess",
    "section": "earthaccess and NASA’s EDL",
    "text": "earthaccess and NASA’s EDL\nWe recommend authenticating your Earthdata Login (EDL) information using the earthaccess python package as follows:\n\nauth = earthaccess.login()"
  },
  {
    "objectID": "notebooks/getting-started.html#lets-plot",
    "href": "notebooks/getting-started.html#lets-plot",
    "title": "Getting Started with earthaccess",
    "section": "Let’s Plot!",
    "text": "Let’s Plot!\n\n%%time\n\nplt.rcParams[\"figure.figsize\"] = (16,4)\n\nfig, axs = plt.subplots()\nplt.grid(True)\n\n#function to get the global mean for plotting\ndef global_mean(SLA, **kwargs):\n    dout=((SLA*ssh_area).sum()/(SLA/SLA*ssh_area).sum())*1000\n    return dout\n\nresult = ds_SSH.SLA.groupby('Time').apply(global_mean)\n\nplt.xlabel('Time (year)',fontsize=16)\nplt.ylabel('Global Mean SLA (meter)',fontsize=12)\nplt.grid(True)\n\nresult.plot(ax=axs, color=\"orange\", marker=\"o\", label='Satellite Record')\n\nhistoric_ts['global_average_sea_level_change'].plot(ax=axs, label='Historical in-situ record', color=\"lightblue\")\n\nplt.legend()\nplt.show()\n\n\n\n\nCPU times: user 4.02 s, sys: 918 ms, total: 4.94 s\nWall time: 3.28 s\n\n\n\nThis Data Story is based on Jinbo Wang’s Earthdata Webinar tutorial."
  }
]